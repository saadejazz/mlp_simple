{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "54875538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fddf9c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activations():\n",
    "    '''\n",
    "    A collection of activation functions.\n",
    "    All activation functions are vector friendly.\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_function(name):\n",
    "        return getattr(Activations, str(name), \"InvalidFunction\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def InvalidFunction(*arg):\n",
    "        raise Exception(\"Invalid activation function\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear(inp, derivative = False):\n",
    "        out = np.array(inp)\n",
    "        if derivative:\n",
    "            out = np.ones(out.shape)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def logistic(inp, derivative = False):\n",
    "        out = np.array(inp)\n",
    "        if derivative:\n",
    "            n_dev = Activations.logistic(out, False)\n",
    "            out = (1 - n_dev) * n_dev\n",
    "        else:\n",
    "            out = 1.0/(1 + np.exp(-1 * inp))\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(inp, derivative = False):\n",
    "        out = np.array(inp)\n",
    "        if derivative:\n",
    "            out = np.where(out >= 0, 1, 0)\n",
    "        else:\n",
    "            out = np.where(out >=0, out, 0)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu(inp, derivative = False, leak = 0.01):\n",
    "        out = np.array(inp)\n",
    "        if derivative:\n",
    "            out = np.where(out >=0, 1, leak)\n",
    "        else:\n",
    "            out = np.where(out >=0, out, leak * out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "eee14047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectionOfFunctions():\n",
    "    '''\n",
    "    A generic collection of functions that has mostly static methods,\n",
    "    and the capability to get attribute corresponding to a string\n",
    "    '''\n",
    "\n",
    "    @staticmethod\n",
    "    def read_function(name):\n",
    "        ''''\n",
    "        returns the method corresponding to the name.\n",
    "        '''\n",
    "        return getattr(Activations, str(name), \"InvalidFunction\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def InvalidFunction(*arg):\n",
    "        '''\n",
    "        raises an exception if the activation function requested does not exist.\n",
    "        '''\n",
    "        raise Exception(\"Invalid activation function\")\n",
    "\n",
    "class Activations(CollectionOfFunctions):\n",
    "    '''\n",
    "    A collection of activation functions written using numpy functions.\n",
    "    Included activation functions are linear, logistic (sigmoid), relu,\n",
    "    and leaky relu. All methods are static, with the capability of \n",
    "    calculating their derivatives.\n",
    "    '''\n",
    "  \n",
    "    @staticmethod\n",
    "    def linear(inp, derivative = False):\n",
    "        '''\n",
    "        returns image of function f(x) = x\n",
    "        '''\n",
    "        out = np.array(inp)\n",
    "        if derivative:\n",
    "            out = np.ones(out.shape)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def logistic(inp, derivative = False):\n",
    "        '''\n",
    "        returns the sigmoid of the input\n",
    "        '''\n",
    "        out = np.array(inp)\n",
    "        if derivative:\n",
    "            n_dev = Activations.logistic(out, False)\n",
    "            out = (1 - n_dev) * n_dev\n",
    "        else:\n",
    "            out = 1.0/(1 + np.exp(-1 * inp))\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(inp, derivative = False):\n",
    "        '''\n",
    "        returns the image of the function f(x) = max(0, x) \n",
    "        '''\n",
    "        out = np.array(inp)\n",
    "        if derivative:\n",
    "            out = np.where(out >= 0, 1, 0)\n",
    "        else:\n",
    "            out = np.where(out >=0, out, 0)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu(inp, derivative = False, leak = 0.01):\n",
    "        '''\n",
    "        returns a modified computation of relu, where negative input yields\n",
    "        a smaller output magnitude than that of when input is positve.\n",
    "        the value of leak is a hyperparameter which may be tuned. \n",
    "        '''\n",
    "        out = np.array(inp)\n",
    "        if derivative:\n",
    "            out = np.where(out >=0, 1, leak)\n",
    "        else:\n",
    "            out = np.where(out >=0, out, leak * out)\n",
    "        return out\n",
    "\n",
    "class Losses(CollectionOfFunctions):\n",
    "    '''\n",
    "    A collection of vector friendly loss functions.\n",
    "    Included loss functions are mean squared error, and binary crosss entropy.\n",
    "    '''\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(expected, outputs):\n",
    "        '''\n",
    "        Mean Square Error loss function\n",
    "        '''\n",
    "        expected = np.array(expected)\n",
    "        outputs = np.array(outputs)\n",
    "        return 0.5 * (expected - outputs) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(expected, outputs, derivative=False):\n",
    "        '''\n",
    "        Cross-entropy loss function\n",
    "        '''\n",
    "        expected = np.array(expected)\n",
    "        outputs = np.array(outputs)\n",
    "        # return -1 * expected * np.log(outputs) - (1 - expected) * np.log(1 - outputs)\n",
    "        return metrics.log_loss(expected, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e022bc22",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Invalid activation function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_796/989410994.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mActivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_796/3016287747.py\u001b[0m in \u001b[0;36mInvalidFunction\u001b[1;34m(*arg)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mraises\u001b[0m \u001b[0man\u001b[0m \u001b[0mexception\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mrequested\u001b[0m \u001b[0mdoes\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mexist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         '''\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid activation function\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mActivations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCollectionOfFunctions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Invalid activation function"
     ]
    }
   ],
   "source": [
    "Activations.InvalidFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "be4564d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    # parameters and activations\n",
    "    net_params = dict()\n",
    "    activations = list()\n",
    "    num_layers = 0\n",
    "    \n",
    "    def __init__(self, structure):\n",
    "        self.create_network(structure)\n",
    "    \n",
    "    def create_network(self, structure):\n",
    "        '''\n",
    "        capability for only dense layers\n",
    "        '''\n",
    "        \n",
    "        # ensure that the first layer is an input layer\n",
    "        try:\n",
    "            assert(structure[0][\"type\"] == \"input\")\n",
    "        except:\n",
    "            raise Exception(\"The first layer needs to be an input layer\")\n",
    "        \n",
    "        self.num_layers = len(structure) - 1\n",
    "        prev_units = int(structure[0][\"units\"])\n",
    "        for i, layer in enumerate(structure[1: ]):\n",
    "            # weight matrix at a layer has (i - 1 x i) shape \n",
    "            current_units = layer[\"units\"]\n",
    "            \n",
    "            # initialize weight and bias paramters\n",
    "            self.net_params[\"w\" + str(i + 1)] = np.zeros((prev_units, current_units))\n",
    "            self.net_params[\"b\" + str(i + 1)] = None\n",
    "            \n",
    "            # add a bias parameter only if specified\n",
    "            if layer[\"bias\"]:\n",
    "                self.net_params[\"b\" + str(i + 1)] = np.zeros((1, layer[\"units\"]))\n",
    "            \n",
    "            # store activation function in attribute and continue loop\n",
    "            self.activations.append(layer[\"activation_function\"])\n",
    "            prev_units = current_units\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def show_model(self):\n",
    "        for name, arr in self.net_params.items():\n",
    "            print(\"Shape of \", name, \": \", arr.shape)\n",
    "            \n",
    "    def initialize_weights(self, leak = 0.01):\n",
    "        '''\n",
    "        Using Kaiming initialization\n",
    "        '''\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            # leak only exists in leaky relu\n",
    "            if self.activations[i - 1] != \"leaky_relu\":\n",
    "                leak = 0\n",
    "            \n",
    "            # using Kaiming initialization only on weights and not biases\n",
    "            param = self.net_params[\"w\" + str(i)]\n",
    "            self.net_params[\"w\" + str(i)] = np.sqrt(2/((1 + leak * leak)\\\n",
    "                                            * param.shape[0])) * np.random.randn(*param.shape)\n",
    "            \n",
    "            # using random initialization on bias if it is included\n",
    "            param = self.net_params[\"b\" + str(i)]\n",
    "            if param is not None:\n",
    "                self.net_params[\"b\" + str(i)] = np.random.randn(*param.shape)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def forward_prop(self, inp):\n",
    "        # sanity check\n",
    "        inp = np.array(inp)\n",
    "        \n",
    "        # propogate\n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            bias = self.net_params[\"b\" + str(l)]\n",
    "            if bias is None:\n",
    "                self.net_params[\"z\" + str(l)] = np.matmul(inp, self.net_params[\"w\" + str(l)])\n",
    "            else:\n",
    "                self.net_params[\"z\" + str(l)] = np.matmul(inp, self.net_params[\"w\" + str(l)]) + bias\n",
    "            act = Activations.read_function(self.activations[l - 1])\n",
    "            self.net_params[\"a\" + str(l)] = act(self.net_params[\"z\" + str(l)])\n",
    "            inp = self.net_params[\"a\" + str(l)]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def back_prop(self, inp, out, loss_name):\n",
    "        # sanity check\n",
    "        inp = np.array(inp)\n",
    "        out = np.array(out)\n",
    "        \n",
    "        # for last layer\n",
    "        # only two types of losses are considered\n",
    "        # ll is the string casted version of number of layers\n",
    "        ll = str(self.num_layers)\n",
    "        if loss_name == \"mse\":\n",
    "            self.net_params[\"delta\" + ll] = np.multiply(self.net_params[\"a\" + ll] - out,\\\n",
    "                                            Activations.read_function(self.activations[-1])\\\n",
    "                                            (self.net_params[\"z\" + ll], derivative = True))\n",
    "        elif loss_name == \"binary_cross_entropy\":\n",
    "            self.net_params[\"delta\" + ll] = self.net_params[\"a\" + ll] - out\n",
    "        else:\n",
    "            raise Exception(\"Only two types of loss functions are supported: MSE and BCE\")\n",
    "        \n",
    "        # for every other layer except the last\n",
    "        for l in reversed(range(1, self.num_layers)):\n",
    "            self.net_params[\"delta\" + str(l)] = np.multiply(Activations.read_function(self.activations[l - 1])\\\n",
    "                                                (self.net_params[\"z\" + str(l)], derivative = True),\\\n",
    "                                                np.matmul(self.net_params[\"delta\" + str(l + 1)],\\\n",
    "                                                np.transpose(self.net_params[\"w\" + str(l + 1)]),))\n",
    "        return self\n",
    "    \n",
    "    def update_weights(self, inp, l_rate):\n",
    "        # update for the first layer\n",
    "        self.net_params[\"dw1\"] = np.matmul(np.transpose(inp), self.net_params[\"delta1\"])\n",
    "        if self.net_params[\"b1\"] is not None:\n",
    "            self.net_params[\"db1\"] = np.sum(self.net_params[\"delta1\"], axis = 0, keepdims = True)\n",
    "        \n",
    "        # calculate gradient of weights\n",
    "        for l in range(2, self.num_layers + 1):\n",
    "            self.net_params[\"dw\" + str(l)] = np.matmul(np.transpose(self.net_params[\"a\" + str(l - 1)]),\\\n",
    "                                             self.net_params[\"delta\" + str(l)])\n",
    "            if self.net_params[\"b\" + str(l)] is not None:\n",
    "                self.net_params[\"db\" + str(l)] = np.sum(self.net_params[\"delta\" + str(l)], axis = 0, keepdims = True)\n",
    "                \n",
    "        # update weights\n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            self.net_params[\"w\" + str(l)] -= l_rate/inp.shape[0] * self.net_params[\"dw\" + str(l)]\n",
    "            if self.net_params[\"b\" + str(l)] is not None:\n",
    "                self.net_params[\"b\" + str(l)] -= l_rate/inp.shape[0] * self.net_params[\"db\" + str(l)]\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def predict(self, inp):\n",
    "        '''\n",
    "        predicts output\n",
    "        '''\n",
    "        self.forward_prop(inp)\n",
    "        result = np.round(self.net_params[\"a\" + str(self.num_layers)])\n",
    "        return result\n",
    "    \n",
    "    def calc_loss(self, out, loss_func):\n",
    "        return np.sum(Losses.read_function(loss_func)(out, self.net_params[\"a\" + str(self.num_layers)]))\n",
    "    \n",
    "    def accuracy(self, inp, out):\n",
    "        '''\n",
    "        returns accuracy as a percentage\n",
    "        '''\n",
    "        return sum(self.predict(inp) == out)[0]/out.shape[0] * 100\n",
    "    \n",
    "    def train(self, X, Y, n_epochs = 500, batch_size = 16, loss_func = \"mse\", l_rate = 0.001, rand_scale = 0.1, verbose = True):\n",
    "        # augmenting data\n",
    "        X = np.concatenate((X, augment_data(X, rand_scale)))\n",
    "        Y = np.concatenate((Y, Y))\n",
    "        \n",
    "        self.initialize_weights()\n",
    "        \n",
    "        num_batches = int(np.ceil(X.shape[0]/batch_size))\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            self.forward_prop(X)\n",
    "            if verbose:\n",
    "                print(\"Loss after \", epoch, \" epoch(s): \", self.calc_loss(Y, loss_func),\\\n",
    "                      \", Accuracy: \", self.accuracy(X, Y))\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                x = X[i * batch_size: (i + 1) * batch_size, :]\n",
    "                y = Y[i * batch_size: (i + 1) * batch_size, :]\n",
    "                \n",
    "                nn.forward_prop(x)\n",
    "                nn.back_prop(x, y, loss_func)\n",
    "                nn.update_weights(x, l_rate)\n",
    "        \n",
    "        self.forward_prop(X)\n",
    "        print(\"Final loss: \", self.calc_loss(Y, loss_func), \"Final accuracy: \", self.accuracy(X, Y))\n",
    "        return self\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "2a768d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = [{'type': 'input', 'units': 2},\n",
    "            {'type': 'dense', 'units': 12, 'activation_function': 'leaky_relu', 'bias': True},\n",
    "            {'type': 'dense', 'units': 12, 'activation_function': 'leaky_relu', 'bias': True},\n",
    "            {'type': 'dense', 'units': 1, 'activation_function': 'logistic', 'bias': True}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "3ae37714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after  0  epoch(s):  2.6750793642772646 , Accuracy:  50.0\n",
      "Loss after  1  epoch(s):  0.9562522486332583 , Accuracy:  44.375\n",
      "Loss after  2  epoch(s):  0.7873867768744078 , Accuracy:  48.75\n",
      "Loss after  3  epoch(s):  0.778651567744058 , Accuracy:  48.75\n",
      "Loss after  4  epoch(s):  0.7693554294307727 , Accuracy:  48.125\n",
      "Loss after  5  epoch(s):  0.7609544215118302 , Accuracy:  48.125\n",
      "Loss after  6  epoch(s):  0.7548363613263613 , Accuracy:  49.375\n",
      "Loss after  7  epoch(s):  0.7503553288577705 , Accuracy:  49.375\n",
      "Loss after  8  epoch(s):  0.7469085951669763 , Accuracy:  50.625\n",
      "Loss after  9  epoch(s):  0.7442399457175497 , Accuracy:  50.625\n",
      "Loss after  10  epoch(s):  0.7422625719809929 , Accuracy:  50.625\n",
      "Loss after  11  epoch(s):  0.7407614363823937 , Accuracy:  50.625\n",
      "Loss after  12  epoch(s):  0.7395994325938623 , Accuracy:  50.625\n",
      "Loss after  13  epoch(s):  0.7386852789152434 , Accuracy:  50.625\n",
      "Loss after  14  epoch(s):  0.7378813972518028 , Accuracy:  49.375\n",
      "Loss after  15  epoch(s):  0.7371936423808739 , Accuracy:  49.375\n",
      "Loss after  16  epoch(s):  0.7365151381024229 , Accuracy:  49.375\n",
      "Loss after  17  epoch(s):  0.735970224547879 , Accuracy:  50.0\n",
      "Loss after  18  epoch(s):  0.7353874914306375 , Accuracy:  50.0\n",
      "Loss after  19  epoch(s):  0.7349032234096619 , Accuracy:  50.0\n",
      "Loss after  20  epoch(s):  0.7343598086441576 , Accuracy:  50.0\n",
      "Loss after  21  epoch(s):  0.7338531540095011 , Accuracy:  50.0\n",
      "Loss after  22  epoch(s):  0.7332751142527881 , Accuracy:  50.0\n",
      "Loss after  23  epoch(s):  0.7328346152240834 , Accuracy:  50.0\n",
      "Loss after  24  epoch(s):  0.7322788580369897 , Accuracy:  50.0\n",
      "Loss after  25  epoch(s):  0.7318291419399967 , Accuracy:  50.0\n",
      "Loss after  26  epoch(s):  0.7312646684806043 , Accuracy:  50.0\n",
      "Loss after  27  epoch(s):  0.7307327292958236 , Accuracy:  50.0\n",
      "Loss after  28  epoch(s):  0.7302075693681352 , Accuracy:  50.0\n",
      "Loss after  29  epoch(s):  0.7297653316755779 , Accuracy:  50.0\n",
      "Loss after  30  epoch(s):  0.7292084160763833 , Accuracy:  50.0\n",
      "Loss after  31  epoch(s):  0.7286813455715304 , Accuracy:  50.0\n",
      "Loss after  32  epoch(s):  0.7281602033945405 , Accuracy:  50.0\n",
      "Loss after  33  epoch(s):  0.7276094989821456 , Accuracy:  50.0\n",
      "Loss after  34  epoch(s):  0.727050973909009 , Accuracy:  50.0\n",
      "Loss after  35  epoch(s):  0.7264296882880755 , Accuracy:  50.0\n",
      "Loss after  36  epoch(s):  0.7256981217109828 , Accuracy:  50.0\n",
      "Loss after  37  epoch(s):  0.7247010631070412 , Accuracy:  50.0\n",
      "Loss after  38  epoch(s):  0.7239787390074588 , Accuracy:  50.0\n",
      "Loss after  39  epoch(s):  0.7233647658480529 , Accuracy:  50.0\n",
      "Loss after  40  epoch(s):  0.7227362328049681 , Accuracy:  50.0\n",
      "Loss after  41  epoch(s):  0.722005627394438 , Accuracy:  50.0\n",
      "Loss after  42  epoch(s):  0.7214632488214922 , Accuracy:  50.0\n",
      "Loss after  43  epoch(s):  0.7205869444563711 , Accuracy:  50.0\n",
      "Loss after  44  epoch(s):  0.7196710325594909 , Accuracy:  50.0\n",
      "Loss after  45  epoch(s):  0.7188156872467928 , Accuracy:  50.0\n",
      "Loss after  46  epoch(s):  0.7180652275572179 , Accuracy:  50.0\n",
      "Loss after  47  epoch(s):  0.7170959666658694 , Accuracy:  50.0\n",
      "Loss after  48  epoch(s):  0.7166317616171749 , Accuracy:  50.0\n",
      "Loss after  49  epoch(s):  0.7160175232444642 , Accuracy:  50.0\n",
      "Loss after  50  epoch(s):  0.7154239308267258 , Accuracy:  50.0\n",
      "Loss after  51  epoch(s):  0.7149162925668311 , Accuracy:  50.0\n",
      "Loss after  52  epoch(s):  0.7143157129688026 , Accuracy:  50.0\n",
      "Loss after  53  epoch(s):  0.7136854493289633 , Accuracy:  50.0\n",
      "Loss after  54  epoch(s):  0.7130494576718688 , Accuracy:  50.0\n",
      "Loss after  55  epoch(s):  0.7125572048174774 , Accuracy:  50.0\n",
      "Loss after  56  epoch(s):  0.7119954536317783 , Accuracy:  50.0\n",
      "Loss after  57  epoch(s):  0.7114142517720735 , Accuracy:  50.0\n",
      "Loss after  58  epoch(s):  0.7108256897363646 , Accuracy:  50.0\n",
      "Loss after  59  epoch(s):  0.7102315476882822 , Accuracy:  50.0\n",
      "Loss after  60  epoch(s):  0.709850363171392 , Accuracy:  50.0\n",
      "Loss after  61  epoch(s):  0.7093780059445611 , Accuracy:  50.0\n",
      "Loss after  62  epoch(s):  0.7085782767118116 , Accuracy:  50.0\n",
      "Loss after  63  epoch(s):  0.7081519048633671 , Accuracy:  50.0\n",
      "Loss after  64  epoch(s):  0.7076674397446621 , Accuracy:  50.0\n",
      "Loss after  65  epoch(s):  0.7070852030892927 , Accuracy:  50.0\n",
      "Loss after  66  epoch(s):  0.7065554880057819 , Accuracy:  50.0\n",
      "Loss after  67  epoch(s):  0.7059677294526046 , Accuracy:  50.0\n",
      "Loss after  68  epoch(s):  0.705446575167586 , Accuracy:  50.0\n",
      "Loss after  69  epoch(s):  0.7048569600565221 , Accuracy:  50.0\n",
      "Loss after  70  epoch(s):  0.7042863905929473 , Accuracy:  50.0\n",
      "Loss after  71  epoch(s):  0.7037523732901543 , Accuracy:  50.0\n",
      "Loss after  72  epoch(s):  0.7031578370398026 , Accuracy:  50.0\n",
      "Loss after  73  epoch(s):  0.7025884637734504 , Accuracy:  50.0\n",
      "Loss after  74  epoch(s):  0.7020213076974352 , Accuracy:  50.0\n",
      "Loss after  75  epoch(s):  0.7015029923866332 , Accuracy:  50.0\n",
      "Loss after  76  epoch(s):  0.7009154326606093 , Accuracy:  50.0\n",
      "Loss after  77  epoch(s):  0.700346544844781 , Accuracy:  50.0\n",
      "Loss after  78  epoch(s):  0.6997772932958138 , Accuracy:  50.0\n",
      "Loss after  79  epoch(s):  0.6992153880071525 , Accuracy:  50.0\n",
      "Loss after  80  epoch(s):  0.6986535564699377 , Accuracy:  50.0\n",
      "Loss after  81  epoch(s):  0.698134320544529 , Accuracy:  50.0\n",
      "Loss after  82  epoch(s):  0.6975567363072457 , Accuracy:  50.0\n",
      "Loss after  83  epoch(s):  0.6969770426104999 , Accuracy:  51.24999999999999\n",
      "Loss after  84  epoch(s):  0.6964094620199142 , Accuracy:  51.24999999999999\n",
      "Loss after  85  epoch(s):  0.6958435559788297 , Accuracy:  51.87500000000001\n",
      "Loss after  86  epoch(s):  0.6952770532466575 , Accuracy:  54.37499999999999\n",
      "Loss after  87  epoch(s):  0.694709472098556 , Accuracy:  54.37499999999999\n",
      "Loss after  88  epoch(s):  0.6941368441226112 , Accuracy:  54.37499999999999\n",
      "Loss after  89  epoch(s):  0.6935658215994857 , Accuracy:  54.37499999999999\n",
      "Loss after  90  epoch(s):  0.6932703332732529 , Accuracy:  54.37499999999999\n",
      "Loss after  91  epoch(s):  0.6924918672637881 , Accuracy:  54.37499999999999\n",
      "Loss after  92  epoch(s):  0.6921610243849275 , Accuracy:  54.37499999999999\n",
      "Loss after  93  epoch(s):  0.6913819315267657 , Accuracy:  54.37499999999999\n",
      "Loss after  94  epoch(s):  0.6910661260524534 , Accuracy:  54.37499999999999\n",
      "Loss after  95  epoch(s):  0.6905675622317384 , Accuracy:  54.37499999999999\n",
      "Loss after  96  epoch(s):  0.6897464945317394 , Accuracy:  55.00000000000001\n",
      "Loss after  97  epoch(s):  0.6894193257356144 , Accuracy:  55.625\n",
      "Loss after  98  epoch(s):  0.6889205328071137 , Accuracy:  55.625\n",
      "Loss after  99  epoch(s):  0.6883727376128979 , Accuracy:  55.625\n",
      "Loss after  100  epoch(s):  0.6878384599681049 , Accuracy:  55.625\n",
      "Loss after  101  epoch(s):  0.6872909941127328 , Accuracy:  55.625\n",
      "Loss after  102  epoch(s):  0.6867386772123927 , Accuracy:  55.625\n",
      "Loss after  103  epoch(s):  0.6861852393869518 , Accuracy:  55.625\n",
      "Loss after  104  epoch(s):  0.6856301443794359 , Accuracy:  55.625\n",
      "Loss after  105  epoch(s):  0.6850735959686246 , Accuracy:  55.625\n",
      "Loss after  106  epoch(s):  0.684515671315323 , Accuracy:  55.625\n",
      "Loss after  107  epoch(s):  0.6839564128902722 , Accuracy:  55.625\n",
      "Loss after  108  epoch(s):  0.6833958501844067 , Accuracy:  55.625\n",
      "Loss after  109  epoch(s):  0.6828340058038375 , Accuracy:  56.25\n",
      "Loss after  110  epoch(s):  0.6822715567202654 , Accuracy:  56.25\n",
      "Loss after  111  epoch(s):  0.681707863479402 , Accuracy:  56.25\n",
      "Loss after  112  epoch(s):  0.6811429071345526 , Accuracy:  56.25\n",
      "Loss after  113  epoch(s):  0.680541058542192 , Accuracy:  56.25\n",
      "Loss after  114  epoch(s):  0.6799964306498547 , Accuracy:  56.875\n",
      "Loss after  115  epoch(s):  0.6794313024791067 , Accuracy:  56.875\n",
      "Loss after  116  epoch(s):  0.6788607493009128 , Accuracy:  56.875\n",
      "Loss after  117  epoch(s):  0.6782551284897453 , Accuracy:  56.875\n",
      "Loss after  118  epoch(s):  0.6777058619467329 , Accuracy:  56.875\n",
      "Loss after  119  epoch(s):  0.6771299399748459 , Accuracy:  56.875\n",
      "Loss after  120  epoch(s):  0.6765175857025997 , Accuracy:  57.49999999999999\n",
      "Loss after  121  epoch(s):  0.6761343527282299 , Accuracy:  57.49999999999999\n",
      "Loss after  122  epoch(s):  0.6754414211200875 , Accuracy:  57.49999999999999\n",
      "Loss after  123  epoch(s):  0.6750183311824159 , Accuracy:  57.49999999999999\n",
      "Loss after  124  epoch(s):  0.6742876479940751 , Accuracy:  57.49999999999999\n",
      "Loss after  125  epoch(s):  0.6738740594692326 , Accuracy:  57.49999999999999\n",
      "Loss after  126  epoch(s):  0.6733540186324609 , Accuracy:  57.49999999999999\n",
      "Loss after  127  epoch(s):  0.6725916146860955 , Accuracy:  57.49999999999999\n",
      "Loss after  128  epoch(s):  0.6721622263870332 , Accuracy:  58.12500000000001\n",
      "Loss after  129  epoch(s):  0.6716381531612875 , Accuracy:  58.12500000000001\n",
      "Loss after  130  epoch(s):  0.6710671148525837 , Accuracy:  58.12500000000001\n",
      "Loss after  131  epoch(s):  0.6705144343822301 , Accuracy:  58.12500000000001\n",
      "Loss after  132  epoch(s):  0.6697177393941243 , Accuracy:  58.12500000000001\n",
      "Loss after  133  epoch(s):  0.6692796062099946 , Accuracy:  58.12500000000001\n",
      "Loss after  134  epoch(s):  0.6687095496148736 , Accuracy:  58.75\n",
      "Loss after  135  epoch(s):  0.6681526990771006 , Accuracy:  58.75\n",
      "Loss after  136  epoch(s):  0.6675682348097525 , Accuracy:  58.75\n",
      "Loss after  137  epoch(s):  0.6669913392772722 , Accuracy:  58.75\n",
      "Loss after  138  epoch(s):  0.666840256508326 , Accuracy:  58.75\n",
      "Loss after  139  epoch(s):  0.6661781646164027 , Accuracy:  58.75\n",
      "Loss after  140  epoch(s):  0.6655967544014444 , Accuracy:  58.75\n",
      "Loss after  141  epoch(s):  0.6650090118431831 , Accuracy:  58.75\n",
      "Loss after  142  epoch(s):  0.6644315008394981 , Accuracy:  58.75\n",
      "Loss after  143  epoch(s):  0.6636547516169785 , Accuracy:  58.75\n",
      "Loss after  144  epoch(s):  0.6630903059055872 , Accuracy:  58.75\n",
      "Loss after  145  epoch(s):  0.6628550844601004 , Accuracy:  58.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after  146  epoch(s):  0.6621316109796181 , Accuracy:  58.75\n",
      "Loss after  147  epoch(s):  0.6614805174248678 , Accuracy:  58.75\n",
      "Loss after  148  epoch(s):  0.6607183123063596 , Accuracy:  58.75\n",
      "Loss after  149  epoch(s):  0.6600900253071803 , Accuracy:  58.75\n",
      "Loss after  150  epoch(s):  0.6594764990933811 , Accuracy:  58.75\n",
      "Loss after  151  epoch(s):  0.658872467263737 , Accuracy:  59.375\n",
      "Loss after  152  epoch(s):  0.6582673161559786 , Accuracy:  59.375\n",
      "Loss after  153  epoch(s):  0.6576593973561087 , Accuracy:  59.375\n",
      "Loss after  154  epoch(s):  0.6570480258482914 , Accuracy:  59.375\n",
      "Loss after  155  epoch(s):  0.6564333654624182 , Accuracy:  59.375\n",
      "Loss after  156  epoch(s):  0.6562943165253834 , Accuracy:  59.375\n",
      "Loss after  157  epoch(s):  0.6554896066678972 , Accuracy:  59.375\n",
      "Loss after  158  epoch(s):  0.6551521073297325 , Accuracy:  59.375\n",
      "Loss after  159  epoch(s):  0.6542970851170604 , Accuracy:  59.375\n",
      "Loss after  160  epoch(s):  0.6539473136626921 , Accuracy:  59.375\n",
      "Loss after  161  epoch(s):  0.653080583544995 , Accuracy:  59.375\n",
      "Loss after  162  epoch(s):  0.6527277780054739 , Accuracy:  59.375\n",
      "Loss after  163  epoch(s):  0.6517180810487618 , Accuracy:  59.375\n",
      "Loss after  164  epoch(s):  0.6514651906510226 , Accuracy:  59.375\n",
      "Loss after  165  epoch(s):  0.6504088003078661 , Accuracy:  59.375\n",
      "Loss after  166  epoch(s):  0.6501662668846814 , Accuracy:  59.375\n",
      "Loss after  167  epoch(s):  0.649614039986707 , Accuracy:  59.375\n",
      "Loss after  168  epoch(s):  0.6486467893651312 , Accuracy:  59.375\n",
      "Loss after  169  epoch(s):  0.6481749040349535 , Accuracy:  59.375\n",
      "Loss after  170  epoch(s):  0.6475747190004295 , Accuracy:  59.375\n",
      "Loss after  171  epoch(s):  0.6469128066900114 , Accuracy:  59.375\n",
      "Loss after  172  epoch(s):  0.646252161269782 , Accuracy:  59.375\n",
      "Loss after  173  epoch(s):  0.6455863755020295 , Accuracy:  59.375\n",
      "Loss after  174  epoch(s):  0.6449442423100397 , Accuracy:  59.375\n",
      "Loss after  175  epoch(s):  0.6443065387007165 , Accuracy:  59.375\n",
      "Loss after  176  epoch(s):  0.6436297130944453 , Accuracy:  59.375\n",
      "Loss after  177  epoch(s):  0.6429600994082111 , Accuracy:  59.375\n",
      "Loss after  178  epoch(s):  0.6422412323794511 , Accuracy:  59.375\n",
      "Loss after  179  epoch(s):  0.6415421543065744 , Accuracy:  60.0\n",
      "Loss after  180  epoch(s):  0.640807141086183 , Accuracy:  60.0\n",
      "Loss after  181  epoch(s):  0.640059824597186 , Accuracy:  60.0\n",
      "Loss after  182  epoch(s):  0.6393201744647962 , Accuracy:  60.0\n",
      "Loss after  183  epoch(s):  0.6386005507130043 , Accuracy:  60.0\n",
      "Loss after  184  epoch(s):  0.6378720689651829 , Accuracy:  60.0\n",
      "Loss after  185  epoch(s):  0.637150390141237 , Accuracy:  60.0\n",
      "Loss after  186  epoch(s):  0.6364290833634515 , Accuracy:  60.0\n",
      "Loss after  187  epoch(s):  0.6357038463229612 , Accuracy:  60.0\n",
      "Loss after  188  epoch(s):  0.6349755583108276 , Accuracy:  59.375\n",
      "Loss after  189  epoch(s):  0.6342720841824431 , Accuracy:  59.375\n",
      "Loss after  190  epoch(s):  0.6335600009395137 , Accuracy:  59.375\n",
      "Loss after  191  epoch(s):  0.6328985528524613 , Accuracy:  59.375\n",
      "Loss after  192  epoch(s):  0.6322226892755684 , Accuracy:  59.375\n",
      "Loss after  193  epoch(s):  0.6318760524763968 , Accuracy:  60.0\n",
      "Loss after  194  epoch(s):  0.6310313039578058 , Accuracy:  59.375\n",
      "Loss after  195  epoch(s):  0.6303238597782572 , Accuracy:  59.375\n",
      "Loss after  196  epoch(s):  0.6299786228970055 , Accuracy:  60.62499999999999\n",
      "Loss after  197  epoch(s):  0.6294487481785289 , Accuracy:  60.62499999999999\n",
      "Loss after  198  epoch(s):  0.628536951515384 , Accuracy:  60.0\n",
      "Loss after  199  epoch(s):  0.6281444602665329 , Accuracy:  60.62499999999999\n",
      "Loss after  200  epoch(s):  0.6275943754111413 , Accuracy:  60.0\n",
      "Loss after  201  epoch(s):  0.6266630541834016 , Accuracy:  60.0\n",
      "Loss after  202  epoch(s):  0.6262635421718686 , Accuracy:  60.0\n",
      "Loss after  203  epoch(s):  0.625704438956425 , Accuracy:  60.0\n",
      "Loss after  204  epoch(s):  0.6251264776144783 , Accuracy:  60.0\n",
      "Loss after  205  epoch(s):  0.6245355193372685 , Accuracy:  60.62499999999999\n",
      "Loss after  206  epoch(s):  0.6239612903104236 , Accuracy:  60.62499999999999\n",
      "Loss after  207  epoch(s):  0.6233720874888162 , Accuracy:  60.62499999999999\n",
      "Loss after  208  epoch(s):  0.6227772370757 , Accuracy:  60.0\n",
      "Loss after  209  epoch(s):  0.6221804001475488 , Accuracy:  60.0\n",
      "Loss after  210  epoch(s):  0.6215830520455873 , Accuracy:  60.0\n",
      "Loss after  211  epoch(s):  0.6210094078547516 , Accuracy:  60.0\n",
      "Loss after  212  epoch(s):  0.6204186322409778 , Accuracy:  60.0\n",
      "Loss after  213  epoch(s):  0.6198232657271514 , Accuracy:  60.0\n",
      "Loss after  214  epoch(s):  0.6192250438685095 , Accuracy:  60.0\n",
      "Loss after  215  epoch(s):  0.6186433080645525 , Accuracy:  60.0\n",
      "Loss after  216  epoch(s):  0.6180599573823216 , Accuracy:  60.0\n",
      "Loss after  217  epoch(s):  0.6183676791381193 , Accuracy:  60.62499999999999\n",
      "Loss after  218  epoch(s):  0.616799292597668 , Accuracy:  60.0\n",
      "Loss after  219  epoch(s):  0.6172092166002223 , Accuracy:  60.62499999999999\n",
      "Loss after  220  epoch(s):  0.6165533854796036 , Accuracy:  61.25000000000001\n",
      "Loss after  221  epoch(s):  0.615069410673003 , Accuracy:  60.62499999999999\n",
      "Loss after  222  epoch(s):  0.6155026689847871 , Accuracy:  61.25000000000001\n",
      "Loss after  223  epoch(s):  0.6148414400738702 , Accuracy:  61.25000000000001\n",
      "Loss after  224  epoch(s):  0.6142556254164737 , Accuracy:  60.62499999999999\n",
      "Loss after  225  epoch(s):  0.6130608738043382 , Accuracy:  60.62499999999999\n",
      "Loss after  226  epoch(s):  0.6123859682131139 , Accuracy:  60.62499999999999\n",
      "Loss after  227  epoch(s):  0.6120137901292025 , Accuracy:  60.62499999999999\n",
      "Loss after  228  epoch(s):  0.6111455391974403 , Accuracy:  60.62499999999999\n",
      "Loss after  229  epoch(s):  0.6107462697390348 , Accuracy:  60.62499999999999\n",
      "Loss after  230  epoch(s):  0.6101647410496774 , Accuracy:  60.62499999999999\n",
      "Loss after  231  epoch(s):  0.6092196792248022 , Accuracy:  60.62499999999999\n",
      "Loss after  232  epoch(s):  0.6088091941592625 , Accuracy:  60.62499999999999\n",
      "Loss after  233  epoch(s):  0.6082474218876746 , Accuracy:  60.62499999999999\n",
      "Loss after  234  epoch(s):  0.6076522634482668 , Accuracy:  60.62499999999999\n",
      "Loss after  235  epoch(s):  0.607045363942228 , Accuracy:  60.62499999999999\n",
      "Loss after  236  epoch(s):  0.6063825937646066 , Accuracy:  61.25000000000001\n",
      "Loss after  237  epoch(s):  0.6056764364634006 , Accuracy:  61.875\n",
      "Loss after  238  epoch(s):  0.6048840071045264 , Accuracy:  61.875\n",
      "Loss after  239  epoch(s):  0.6040234842463583 , Accuracy:  61.875\n",
      "Loss after  240  epoch(s):  0.6027787835604779 , Accuracy:  61.875\n",
      "Loss after  241  epoch(s):  0.602115199165307 , Accuracy:  61.875\n",
      "Loss after  242  epoch(s):  0.6013399418173767 , Accuracy:  61.875\n",
      "Loss after  243  epoch(s):  0.6005142365143875 , Accuracy:  62.5\n",
      "Loss after  244  epoch(s):  0.5997758081524334 , Accuracy:  63.125\n",
      "Loss after  245  epoch(s):  0.5990041981660014 , Accuracy:  63.74999999999999\n",
      "Loss after  246  epoch(s):  0.5982356078135388 , Accuracy:  63.74999999999999\n",
      "Loss after  247  epoch(s):  0.5974698194292752 , Accuracy:  63.74999999999999\n",
      "Loss after  248  epoch(s):  0.5967801308239833 , Accuracy:  63.74999999999999\n",
      "Loss after  249  epoch(s):  0.5960782140809521 , Accuracy:  63.74999999999999\n",
      "Loss after  250  epoch(s):  0.595364852990674 , Accuracy:  63.74999999999999\n",
      "Loss after  251  epoch(s):  0.5946271472856441 , Accuracy:  63.74999999999999\n",
      "Loss after  252  epoch(s):  0.5939082212788527 , Accuracy:  63.74999999999999\n",
      "Loss after  253  epoch(s):  0.5931695491455152 , Accuracy:  65.0\n",
      "Loss after  254  epoch(s):  0.5923744917106327 , Accuracy:  65.0\n",
      "Loss after  255  epoch(s):  0.5915996648157434 , Accuracy:  65.625\n",
      "Loss after  256  epoch(s):  0.590824060843459 , Accuracy:  65.625\n",
      "Loss after  257  epoch(s):  0.5900392034908151 , Accuracy:  65.625\n",
      "Loss after  258  epoch(s):  0.5898709057527494 , Accuracy:  65.625\n",
      "Loss after  259  epoch(s):  0.5885665721970413 , Accuracy:  66.875\n",
      "Loss after  260  epoch(s):  0.5876650944901242 , Accuracy:  67.5\n",
      "Loss after  261  epoch(s):  0.5868845167440471 , Accuracy:  67.5\n",
      "Loss after  262  epoch(s):  0.5860927634600372 , Accuracy:  67.5\n",
      "Loss after  263  epoch(s):  0.5859426335295514 , Accuracy:  67.5\n",
      "Loss after  264  epoch(s):  0.5847048443972864 , Accuracy:  67.5\n",
      "Loss after  265  epoch(s):  0.5845175578277274 , Accuracy:  67.5\n",
      "Loss after  266  epoch(s):  0.5832324504847197 , Accuracy:  67.5\n",
      "Loss after  267  epoch(s):  0.5830332914343229 , Accuracy:  67.5\n",
      "Loss after  268  epoch(s):  0.5817443456542933 , Accuracy:  67.5\n",
      "Loss after  269  epoch(s):  0.5815754201021346 , Accuracy:  67.5\n",
      "Loss after  270  epoch(s):  0.5809627960803276 , Accuracy:  67.5\n",
      "Loss after  271  epoch(s):  0.580240036501294 , Accuracy:  67.5\n",
      "Loss after  272  epoch(s):  0.5793969999368512 , Accuracy:  67.5\n",
      "Loss after  273  epoch(s):  0.5777944293233205 , Accuracy:  67.5\n",
      "Loss after  274  epoch(s):  0.5771751078598275 , Accuracy:  67.5\n",
      "Loss after  275  epoch(s):  0.5755523170307273 , Accuracy:  67.5\n",
      "Loss after  276  epoch(s):  0.5745060721230018 , Accuracy:  67.5\n",
      "Loss after  277  epoch(s):  0.5735470640405339 , Accuracy:  67.5\n",
      "Loss after  278  epoch(s):  0.5724276093761311 , Accuracy:  67.5\n",
      "Loss after  279  epoch(s):  0.5712968988563121 , Accuracy:  67.5\n",
      "Loss after  280  epoch(s):  0.5702149903975807 , Accuracy:  67.5\n",
      "Loss after  281  epoch(s):  0.5692267359049217 , Accuracy:  67.5\n",
      "Loss after  282  epoch(s):  0.5683154232629559 , Accuracy:  67.5\n",
      "Loss after  283  epoch(s):  0.567320228887674 , Accuracy:  67.5\n",
      "Loss after  284  epoch(s):  0.5664438730501344 , Accuracy:  67.5\n",
      "Loss after  285  epoch(s):  0.5654557517059988 , Accuracy:  67.5\n",
      "Loss after  286  epoch(s):  0.5645371563662822 , Accuracy:  67.5\n",
      "Loss after  287  epoch(s):  0.5635134579313151 , Accuracy:  67.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after  288  epoch(s):  0.5625263512447363 , Accuracy:  67.5\n",
      "Loss after  289  epoch(s):  0.5613968877267171 , Accuracy:  68.75\n",
      "Loss after  290  epoch(s):  0.5605441654003572 , Accuracy:  69.375\n",
      "Loss after  291  epoch(s):  0.5595730117718233 , Accuracy:  69.375\n",
      "Loss after  292  epoch(s):  0.5587390662590552 , Accuracy:  69.375\n",
      "Loss after  293  epoch(s):  0.5579014689405015 , Accuracy:  69.375\n",
      "Loss after  294  epoch(s):  0.5560698921671829 , Accuracy:  69.375\n",
      "Loss after  295  epoch(s):  0.5561541927493726 , Accuracy:  69.375\n",
      "Loss after  296  epoch(s):  0.5554780820526898 , Accuracy:  69.375\n",
      "Loss after  297  epoch(s):  0.5547034849005658 , Accuracy:  69.375\n",
      "Loss after  298  epoch(s):  0.554052316843174 , Accuracy:  69.375\n",
      "Loss after  299  epoch(s):  0.5533011564833638 , Accuracy:  69.375\n",
      "Loss after  300  epoch(s):  0.5525836519327783 , Accuracy:  69.375\n",
      "Loss after  301  epoch(s):  0.5518501082121438 , Accuracy:  69.375\n",
      "Loss after  302  epoch(s):  0.5510771694472792 , Accuracy:  69.375\n",
      "Loss after  303  epoch(s):  0.5503578525910969 , Accuracy:  69.375\n",
      "Loss after  304  epoch(s):  0.5496032981445824 , Accuracy:  69.375\n",
      "Loss after  305  epoch(s):  0.5488654781902225 , Accuracy:  69.375\n",
      "Loss after  306  epoch(s):  0.5480902506810106 , Accuracy:  69.375\n",
      "Loss after  307  epoch(s):  0.5473252430864379 , Accuracy:  69.375\n",
      "Loss after  308  epoch(s):  0.5465836516639424 , Accuracy:  69.375\n",
      "Loss after  309  epoch(s):  0.5458063814563976 , Accuracy:  69.375\n",
      "Loss after  310  epoch(s):  0.5450397466422304 , Accuracy:  69.375\n",
      "Loss after  311  epoch(s):  0.5442977411401516 , Accuracy:  70.0\n",
      "Loss after  312  epoch(s):  0.5435195788847414 , Accuracy:  70.0\n",
      "Loss after  313  epoch(s):  0.5427527040404079 , Accuracy:  70.0\n",
      "Loss after  314  epoch(s):  0.5420117686764715 , Accuracy:  70.0\n",
      "Loss after  315  epoch(s):  0.541233721204283 , Accuracy:  70.0\n",
      "Loss after  316  epoch(s):  0.5404701268891798 , Accuracy:  70.0\n",
      "Loss after  317  epoch(s):  0.5397324792251045 , Accuracy:  70.0\n",
      "Loss after  318  epoch(s):  0.5389597582521428 , Accuracy:  70.0\n",
      "Loss after  319  epoch(s):  0.5382003032154848 , Accuracy:  70.0\n",
      "Loss after  320  epoch(s):  0.5374692012509584 , Accuracy:  70.0\n",
      "Loss after  321  epoch(s):  0.536700876884112 , Accuracy:  70.0\n",
      "Loss after  322  epoch(s):  0.5359476605230523 , Accuracy:  70.625\n",
      "Loss after  323  epoch(s):  0.5352225894599009 , Accuracy:  70.625\n",
      "Loss after  324  epoch(s):  0.5344567342190812 , Accuracy:  70.625\n",
      "Loss after  325  epoch(s):  0.533770965294442 , Accuracy:  71.25\n",
      "Loss after  326  epoch(s):  0.5329579821117589 , Accuracy:  71.25\n",
      "Loss after  327  epoch(s):  0.5322453144882822 , Accuracy:  71.25\n",
      "Loss after  328  epoch(s):  0.5315158755305938 , Accuracy:  71.25\n",
      "Loss after  329  epoch(s):  0.5306975690352992 , Accuracy:  71.25\n",
      "Loss after  330  epoch(s):  0.5299868284503562 , Accuracy:  71.25\n",
      "Loss after  331  epoch(s):  0.5296248333855945 , Accuracy:  71.25\n",
      "Loss after  332  epoch(s):  0.5286010743550593 , Accuracy:  71.25\n",
      "Loss after  333  epoch(s):  0.5282046666910236 , Accuracy:  71.25\n",
      "Loss after  334  epoch(s):  0.5271247844639598 , Accuracy:  72.5\n",
      "Loss after  335  epoch(s):  0.526775315231716 , Accuracy:  72.5\n",
      "Loss after  336  epoch(s):  0.525740384213089 , Accuracy:  72.5\n",
      "Loss after  337  epoch(s):  0.5253386601350523 , Accuracy:  72.5\n",
      "Loss after  338  epoch(s):  0.5246609927900805 , Accuracy:  72.5\n",
      "Loss after  339  epoch(s):  0.5236396666210661 , Accuracy:  72.5\n",
      "Loss after  340  epoch(s):  0.5231832074740653 , Accuracy:  72.5\n",
      "Loss after  341  epoch(s):  0.5225466849284063 , Accuracy:  73.125\n",
      "Loss after  342  epoch(s):  0.5218379261038787 , Accuracy:  73.125\n",
      "Loss after  343  epoch(s):  0.5211776979414084 , Accuracy:  73.125\n",
      "Loss after  344  epoch(s):  0.5201233575673931 , Accuracy:  73.125\n",
      "Loss after  345  epoch(s):  0.5196527186004433 , Accuracy:  73.125\n",
      "Loss after  346  epoch(s):  0.5190102804775558 , Accuracy:  73.125\n",
      "Loss after  347  epoch(s):  0.5183253767121718 , Accuracy:  73.125\n",
      "Loss after  348  epoch(s):  0.5176339984243461 , Accuracy:  73.125\n",
      "Loss after  349  epoch(s):  0.5169047017383724 , Accuracy:  73.125\n",
      "Loss after  350  epoch(s):  0.5162321280987852 , Accuracy:  73.125\n",
      "Loss after  351  epoch(s):  0.5155278466697342 , Accuracy:  73.125\n",
      "Loss after  352  epoch(s):  0.5148257779496651 , Accuracy:  73.125\n",
      "Loss after  353  epoch(s):  0.5141219208082383 , Accuracy:  73.125\n",
      "Loss after  354  epoch(s):  0.5133831656909538 , Accuracy:  73.125\n",
      "Loss after  355  epoch(s):  0.5127060808505859 , Accuracy:  73.125\n",
      "Loss after  356  epoch(s):  0.5120060538655433 , Accuracy:  73.125\n",
      "Loss after  357  epoch(s):  0.511302806495709 , Accuracy:  73.125\n",
      "Loss after  358  epoch(s):  0.5105982070742859 , Accuracy:  73.125\n",
      "Loss after  359  epoch(s):  0.5098927890101899 , Accuracy:  73.125\n",
      "Loss after  360  epoch(s):  0.5091869057678047 , Accuracy:  73.125\n",
      "Loss after  361  epoch(s):  0.508445704843407 , Accuracy:  73.125\n",
      "Loss after  362  epoch(s):  0.5077706059450058 , Accuracy:  73.125\n",
      "Loss after  363  epoch(s):  0.5070720919087017 , Accuracy:  73.125\n",
      "Loss after  364  epoch(s):  0.506371421351556 , Accuracy:  73.125\n",
      "Loss after  365  epoch(s):  0.5056709075573249 , Accuracy:  73.125\n",
      "Loss after  366  epoch(s):  0.5049702815499749 , Accuracy:  73.125\n",
      "Loss after  367  epoch(s):  0.5042698121431677 , Accuracy:  73.125\n",
      "Loss after  368  epoch(s):  0.5035697029354779 , Accuracy:  73.125\n",
      "Loss after  369  epoch(s):  0.502870113206963 , Accuracy:  73.125\n",
      "Loss after  370  epoch(s):  0.5021714164659304 , Accuracy:  73.75\n",
      "Loss after  371  epoch(s):  0.5014742147462118 , Accuracy:  73.75\n",
      "Loss after  372  epoch(s):  0.5009888066302166 , Accuracy:  73.75\n",
      "Loss after  373  epoch(s):  0.5000887615138842 , Accuracy:  73.75\n",
      "Loss after  374  epoch(s):  0.499637295523382 , Accuracy:  73.75\n",
      "Loss after  375  epoch(s):  0.49870794723163814 , Accuracy:  73.75\n",
      "Loss after  376  epoch(s):  0.49825747001028853 , Accuracy:  73.75\n",
      "Loss after  377  epoch(s):  0.4973247306907263 , Accuracy:  73.75\n",
      "Loss after  378  epoch(s):  0.4968778253235609 , Accuracy:  73.75\n",
      "Loss after  379  epoch(s):  0.4959428446064364 , Accuracy:  73.75\n",
      "Loss after  380  epoch(s):  0.49549723626403763 , Accuracy:  73.75\n",
      "Loss after  381  epoch(s):  0.49455865524216336 , Accuracy:  74.375\n",
      "Loss after  382  epoch(s):  0.49385933324668124 , Accuracy:  75.0\n",
      "Loss after  383  epoch(s):  0.49343888534219965 , Accuracy:  75.0\n",
      "Loss after  384  epoch(s):  0.4924982481636421 , Accuracy:  75.0\n",
      "Loss after  385  epoch(s):  0.4918022630814777 , Accuracy:  75.0\n",
      "Loss after  386  epoch(s):  0.49139192492215694 , Accuracy:  75.0\n",
      "Loss after  387  epoch(s):  0.49044938388570064 , Accuracy:  75.0\n",
      "Loss after  388  epoch(s):  0.49003131441185666 , Accuracy:  75.0\n",
      "Loss after  389  epoch(s):  0.48905613841586754 , Accuracy:  75.0\n",
      "Loss after  390  epoch(s):  0.48834447883709375 , Accuracy:  75.0\n",
      "Loss after  391  epoch(s):  0.4879310320332939 , Accuracy:  75.0\n",
      "Loss after  392  epoch(s):  0.4869279281474969 , Accuracy:  75.0\n",
      "Loss after  393  epoch(s):  0.4864852558917335 , Accuracy:  75.0\n",
      "Loss after  394  epoch(s):  0.48550155725596633 , Accuracy:  75.0\n",
      "Loss after  395  epoch(s):  0.48479295905219233 , Accuracy:  75.0\n",
      "Loss after  396  epoch(s):  0.4844297698429897 , Accuracy:  75.0\n",
      "Loss after  397  epoch(s):  0.48348750994058964 , Accuracy:  75.0\n",
      "Loss after  398  epoch(s):  0.4828171402226392 , Accuracy:  75.0\n",
      "Loss after  399  epoch(s):  0.4821736934417805 , Accuracy:  75.0\n",
      "Loss after  400  epoch(s):  0.4818839513664107 , Accuracy:  75.0\n",
      "Loss after  401  epoch(s):  0.48096475286229373 , Accuracy:  75.625\n",
      "Loss after  402  epoch(s):  0.4803193895690175 , Accuracy:  75.625\n",
      "Loss after  403  epoch(s):  0.4799930198919154 , Accuracy:  75.625\n",
      "Loss after  404  epoch(s):  0.4790627233235864 , Accuracy:  75.625\n",
      "Loss after  405  epoch(s):  0.4784140098183979 , Accuracy:  76.25\n",
      "Loss after  406  epoch(s):  0.4780934635400209 , Accuracy:  76.25\n",
      "Loss after  407  epoch(s):  0.4771612201033727 , Accuracy:  76.25\n",
      "Loss after  408  epoch(s):  0.47651889609016607 , Accuracy:  76.25\n",
      "Loss after  409  epoch(s):  0.4762114812816778 , Accuracy:  76.25\n",
      "Loss after  410  epoch(s):  0.4752756141686609 , Accuracy:  76.25\n",
      "Loss after  411  epoch(s):  0.47463557577345483 , Accuracy:  76.25\n",
      "Loss after  412  epoch(s):  0.4743379198796033 , Accuracy:  76.25\n",
      "Loss after  413  epoch(s):  0.47339802980379064 , Accuracy:  76.25\n",
      "Loss after  414  epoch(s):  0.47309005834381557 , Accuracy:  76.25\n",
      "Loss after  415  epoch(s):  0.4721494602701851 , Accuracy:  76.25\n",
      "Loss after  416  epoch(s):  0.47151266900693845 , Accuracy:  76.25\n",
      "Loss after  417  epoch(s):  0.4712299772138252 , Accuracy:  76.25\n",
      "Loss after  418  epoch(s):  0.4702873728535497 , Accuracy:  76.25\n",
      "Loss after  419  epoch(s):  0.4699932980642414 , Accuracy:  76.25\n",
      "Loss after  420  epoch(s):  0.46905141330209676 , Accuracy:  76.25\n",
      "Loss after  421  epoch(s):  0.46842258639390655 , Accuracy:  76.875\n",
      "Loss after  422  epoch(s):  0.46815737474785346 , Accuracy:  76.875\n",
      "Loss after  423  epoch(s):  0.467212938300241 , Accuracy:  76.875\n",
      "Loss after  424  epoch(s):  0.46693798432509415 , Accuracy:  76.875\n",
      "Loss after  425  epoch(s):  0.46599318434738635 , Accuracy:  76.875\n",
      "Loss after  426  epoch(s):  0.46572202326293316 , Accuracy:  76.875\n",
      "Loss after  427  epoch(s):  0.46477769195025165 , Accuracy:  76.875\n",
      "Loss after  428  epoch(s):  0.4641581100995598 , Accuracy:  76.875\n",
      "Loss after  429  epoch(s):  0.46391086956900285 , Accuracy:  76.875\n",
      "Loss after  430  epoch(s):  0.4629574516799851 , Accuracy:  76.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after  431  epoch(s):  0.46269915286112007 , Accuracy:  76.875\n",
      "Loss after  432  epoch(s):  0.4617093194520135 , Accuracy:  76.875\n",
      "Loss after  433  epoch(s):  0.4613983079273899 , Accuracy:  76.875\n",
      "Loss after  434  epoch(s):  0.4603135556490227 , Accuracy:  76.875\n",
      "Loss after  435  epoch(s):  0.45985926781028114 , Accuracy:  76.875\n",
      "Loss after  436  epoch(s):  0.45894389631688454 , Accuracy:  76.875\n",
      "Loss after  437  epoch(s):  0.45831095061142674 , Accuracy:  76.875\n",
      "Loss after  438  epoch(s):  0.4580651330947211 , Accuracy:  76.875\n",
      "Loss after  439  epoch(s):  0.4570305500724469 , Accuracy:  77.5\n",
      "Loss after  440  epoch(s):  0.45673128256710943 , Accuracy:  77.5\n",
      "Loss after  441  epoch(s):  0.45571116339209167 , Accuracy:  77.5\n",
      "Loss after  442  epoch(s):  0.45504408675023045 , Accuracy:  77.5\n",
      "Loss after  443  epoch(s):  0.4547439079051399 , Accuracy:  77.5\n",
      "Loss after  444  epoch(s):  0.4539007808521706 , Accuracy:  77.5\n",
      "Loss after  445  epoch(s):  0.45331442423928553 , Accuracy:  77.5\n",
      "Loss after  446  epoch(s):  0.4531230961765841 , Accuracy:  77.5\n",
      "Loss after  447  epoch(s):  0.45217023913184384 , Accuracy:  77.5\n",
      "Loss after  448  epoch(s):  0.451579086086773 , Accuracy:  77.5\n",
      "Loss after  449  epoch(s):  0.4513927415226903 , Accuracy:  77.5\n",
      "Loss after  450  epoch(s):  0.4504345707756411 , Accuracy:  77.5\n",
      "Loss after  451  epoch(s):  0.4502325939947469 , Accuracy:  77.5\n",
      "Loss after  452  epoch(s):  0.4492714480280906 , Accuracy:  77.5\n",
      "Loss after  453  epoch(s):  0.44867655901798303 , Accuracy:  78.125\n",
      "Loss after  454  epoch(s):  0.4485252333613149 , Accuracy:  78.125\n",
      "Loss after  455  epoch(s):  0.44755019199492674 , Accuracy:  78.75\n",
      "Loss after  456  epoch(s):  0.44736673794667114 , Accuracy:  78.75\n",
      "Loss after  457  epoch(s):  0.446408732132021 , Accuracy:  78.75\n",
      "Loss after  458  epoch(s):  0.4462615142184907 , Accuracy:  78.75\n",
      "Loss after  459  epoch(s):  0.4452853595539338 , Accuracy:  78.75\n",
      "Loss after  460  epoch(s):  0.4451389618379985 , Accuracy:  78.75\n",
      "Loss after  461  epoch(s):  0.4441605882874245 , Accuracy:  78.75\n",
      "Loss after  462  epoch(s):  0.44399068333668057 , Accuracy:  78.75\n",
      "Loss after  463  epoch(s):  0.4430635081104062 , Accuracy:  78.75\n",
      "Loss after  464  epoch(s):  0.4428863753695841 , Accuracy:  79.375\n",
      "Loss after  465  epoch(s):  0.44192939575813595 , Accuracy:  80.0\n",
      "Loss after  466  epoch(s):  0.44181287636971084 , Accuracy:  80.0\n",
      "Loss after  467  epoch(s):  0.44083873148194536 , Accuracy:  80.0\n",
      "Loss after  468  epoch(s):  0.44072744871729663 , Accuracy:  80.0\n",
      "Loss after  469  epoch(s):  0.43975103492322454 , Accuracy:  80.0\n",
      "Loss after  470  epoch(s):  0.4396178185677724 , Accuracy:  80.0\n",
      "Loss after  471  epoch(s):  0.4391110198352425 , Accuracy:  80.625\n",
      "Loss after  472  epoch(s):  0.43812672695735344 , Accuracy:  80.625\n",
      "Loss after  473  epoch(s):  0.4380251396706146 , Accuracy:  80.625\n",
      "Loss after  474  epoch(s):  0.43705051853400556 , Accuracy:  80.625\n",
      "Loss after  475  epoch(s):  0.43695988188113744 , Accuracy:  80.625\n",
      "Loss after  476  epoch(s):  0.43598478545799 , Accuracy:  81.25\n",
      "Loss after  477  epoch(s):  0.43590289788867775 , Accuracy:  80.625\n",
      "Loss after  478  epoch(s):  0.4353626583432494 , Accuracy:  81.25\n",
      "Loss after  479  epoch(s):  0.43439607521745527 , Accuracy:  81.25\n",
      "Loss after  480  epoch(s):  0.43432211252476466 , Accuracy:  81.25\n",
      "Loss after  481  epoch(s):  0.4333501185623764 , Accuracy:  81.25\n",
      "Loss after  482  epoch(s):  0.4332855493702185 , Accuracy:  81.25\n",
      "Loss after  483  epoch(s):  0.4327558754184498 , Accuracy:  81.25\n",
      "Loss after  484  epoch(s):  0.4318229037830311 , Accuracy:  81.875\n",
      "Loss after  485  epoch(s):  0.4317164085508628 , Accuracy:  81.875\n",
      "Loss after  486  epoch(s):  0.4308733199587751 , Accuracy:  82.5\n",
      "Loss after  487  epoch(s):  0.43078170408240996 , Accuracy:  82.5\n",
      "Loss after  488  epoch(s):  0.4303064908307862 , Accuracy:  82.5\n",
      "Loss after  489  epoch(s):  0.4293312469151879 , Accuracy:  82.5\n",
      "Loss after  490  epoch(s):  0.4292962533760557 , Accuracy:  82.5\n",
      "Loss after  491  epoch(s):  0.42878587929070305 , Accuracy:  82.5\n",
      "Loss after  492  epoch(s):  0.42786087865456857 , Accuracy:  82.5\n",
      "Loss after  493  epoch(s):  0.4277861067493715 , Accuracy:  82.5\n",
      "Loss after  494  epoch(s):  0.42686886155465453 , Accuracy:  82.5\n",
      "Loss after  495  epoch(s):  0.4268375411282509 , Accuracy:  82.5\n",
      "Loss after  496  epoch(s):  0.42633503643737913 , Accuracy:  82.5\n",
      "Loss after  497  epoch(s):  0.42540781575245123 , Accuracy:  82.5\n",
      "Loss after  498  epoch(s):  0.4253520765552709 , Accuracy:  82.5\n",
      "Loss after  499  epoch(s):  0.4249021656660104 , Accuracy:  82.5\n",
      "Loss after  500  epoch(s):  0.42392976343435096 , Accuracy:  82.5\n",
      "Loss after  501  epoch(s):  0.42393106762192423 , Accuracy:  82.5\n",
      "Loss after  502  epoch(s):  0.42344096957814126 , Accuracy:  82.5\n",
      "Loss after  503  epoch(s):  0.4225194545155551 , Accuracy:  82.5\n",
      "Loss after  504  epoch(s):  0.42251380522829524 , Accuracy:  82.5\n",
      "Loss after  505  epoch(s):  0.4220269569547074 , Accuracy:  82.5\n",
      "Loss after  506  epoch(s):  0.421105529076183 , Accuracy:  82.5\n",
      "Loss after  507  epoch(s):  0.4210770978566069 , Accuracy:  82.5\n",
      "Loss after  508  epoch(s):  0.4206437909709989 , Accuracy:  82.5\n",
      "Loss after  509  epoch(s):  0.41970881862768544 , Accuracy:  82.5\n",
      "Loss after  510  epoch(s):  0.4196883669491339 , Accuracy:  82.5\n",
      "Loss after  511  epoch(s):  0.41925992279070945 , Accuracy:  82.5\n",
      "Loss after  512  epoch(s):  0.41829392431465673 , Accuracy:  82.5\n",
      "Loss after  513  epoch(s):  0.41931566136940557 , Accuracy:  82.5\n",
      "Loss after  514  epoch(s):  0.41835978369117405 , Accuracy:  82.5\n",
      "Loss after  515  epoch(s):  0.41832857533798995 , Accuracy:  82.5\n",
      "Loss after  516  epoch(s):  0.41773474975913844 , Accuracy:  82.5\n",
      "Loss after  517  epoch(s):  0.41725656439348346 , Accuracy:  82.5\n",
      "Loss after  518  epoch(s):  0.41727363103541465 , Accuracy:  82.5\n",
      "Loss after  519  epoch(s):  0.4163613967333081 , Accuracy:  82.5\n",
      "Loss after  520  epoch(s):  0.4158672260355495 , Accuracy:  82.5\n",
      "Loss after  521  epoch(s):  0.41542399513733475 , Accuracy:  82.5\n",
      "Loss after  522  epoch(s):  0.4154914276069248 , Accuracy:  82.5\n",
      "Loss after  523  epoch(s):  0.4145523759879814 , Accuracy:  82.5\n",
      "Loss after  524  epoch(s):  0.4141076835951578 , Accuracy:  82.5\n",
      "Loss after  525  epoch(s):  0.41420923327607484 , Accuracy:  82.5\n",
      "Loss after  526  epoch(s):  0.4133107614727457 , Accuracy:  82.5\n",
      "Loss after  527  epoch(s):  0.4128590669260335 , Accuracy:  82.5\n",
      "Loss after  528  epoch(s):  0.41292271068663833 , Accuracy:  82.5\n",
      "Loss after  529  epoch(s):  0.41202217825017745 , Accuracy:  82.5\n",
      "Loss after  530  epoch(s):  0.41155324535097737 , Accuracy:  82.5\n",
      "Loss after  531  epoch(s):  0.4116603483558812 , Accuracy:  82.5\n",
      "Loss after  532  epoch(s):  0.4107502671292698 , Accuracy:  82.5\n",
      "Loss after  533  epoch(s):  0.410285333017839 , Accuracy:  82.5\n",
      "Loss after  534  epoch(s):  0.41040031481025785 , Accuracy:  82.5\n",
      "Loss after  535  epoch(s):  0.4094690055585654 , Accuracy:  82.5\n",
      "Loss after  536  epoch(s):  0.409044343872471 , Accuracy:  82.5\n",
      "Loss after  537  epoch(s):  0.40915497419016766 , Accuracy:  82.5\n",
      "Loss after  538  epoch(s):  0.40822401269228054 , Accuracy:  82.5\n",
      "Loss after  539  epoch(s):  0.40780346177807514 , Accuracy:  82.5\n",
      "Loss after  540  epoch(s):  0.4079223403781363 , Accuracy:  82.5\n",
      "Loss after  541  epoch(s):  0.4069919509019 , Accuracy:  82.5\n",
      "Loss after  542  epoch(s):  0.40658282623776615 , Accuracy:  82.5\n",
      "Loss after  543  epoch(s):  0.4067083334472029 , Accuracy:  82.5\n",
      "Loss after  544  epoch(s):  0.4057767615764439 , Accuracy:  82.5\n",
      "Loss after  545  epoch(s):  0.4059099445016231 , Accuracy:  82.5\n",
      "Loss after  546  epoch(s):  0.4050052461960429 , Accuracy:  82.5\n",
      "Loss after  547  epoch(s):  0.4045552804283486 , Accuracy:  82.5\n",
      "Loss after  548  epoch(s):  0.40470664312129784 , Accuracy:  82.5\n",
      "Loss after  549  epoch(s):  0.40380313528157696 , Accuracy:  82.5\n",
      "Loss after  550  epoch(s):  0.40391398213049773 , Accuracy:  82.5\n",
      "Loss after  551  epoch(s):  0.4030261505603674 , Accuracy:  82.5\n",
      "Loss after  552  epoch(s):  0.4026107366325286 , Accuracy:  82.5\n",
      "Loss after  553  epoch(s):  0.4027385749407504 , Accuracy:  82.5\n",
      "Loss after  554  epoch(s):  0.4018500754650415 , Accuracy:  82.5\n",
      "Loss after  555  epoch(s):  0.4014404280795453 , Accuracy:  82.5\n",
      "Loss after  556  epoch(s):  0.4015776171377424 , Accuracy:  82.5\n",
      "Loss after  557  epoch(s):  0.4006880012876243 , Accuracy:  82.5\n",
      "Loss after  558  epoch(s):  0.4008416859168834 , Accuracy:  82.5\n",
      "Loss after  559  epoch(s):  0.3999199335585815 , Accuracy:  82.5\n",
      "Loss after  560  epoch(s):  0.39952873106921993 , Accuracy:  82.5\n",
      "Loss after  561  epoch(s):  0.39969953818720655 , Accuracy:  82.5\n",
      "Loss after  562  epoch(s):  0.39880003928975966 , Accuracy:  82.5\n",
      "Loss after  563  epoch(s):  0.3989422644658823 , Accuracy:  82.5\n",
      "Loss after  564  epoch(s):  0.3980574603636313 , Accuracy:  82.5\n",
      "Loss after  565  epoch(s):  0.39822436534015776 , Accuracy:  82.5\n",
      "Loss after  566  epoch(s):  0.39730674237709074 , Accuracy:  82.5\n",
      "Loss after  567  epoch(s):  0.3969099241432796 , Accuracy:  82.5\n",
      "Loss after  568  epoch(s):  0.39709405914225054 , Accuracy:  82.5\n",
      "Loss after  569  epoch(s):  0.39620033855154185 , Accuracy:  82.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after  570  epoch(s):  0.39635488352276843 , Accuracy:  82.5\n",
      "Loss after  571  epoch(s):  0.39547685567373936 , Accuracy:  82.5\n",
      "Loss after  572  epoch(s):  0.39508820027500235 , Accuracy:  82.5\n",
      "Loss after  573  epoch(s):  0.3952806200723027 , Accuracy:  82.5\n",
      "Loss after  574  epoch(s):  0.3941972443052447 , Accuracy:  82.5\n",
      "Loss after  575  epoch(s):  0.39381748389361865 , Accuracy:  82.5\n",
      "Loss after  576  epoch(s):  0.3940143771905288 , Accuracy:  82.5\n",
      "Loss after  577  epoch(s):  0.3931240246973195 , Accuracy:  82.5\n",
      "Loss after  578  epoch(s):  0.3927428778879998 , Accuracy:  82.5\n",
      "Loss after  579  epoch(s):  0.392217575440522 , Accuracy:  82.5\n",
      "Loss after  580  epoch(s):  0.39092548218477935 , Accuracy:  82.5\n",
      "Loss after  581  epoch(s):  0.3899870352591207 , Accuracy:  82.5\n",
      "Loss after  582  epoch(s):  0.3895497840551757 , Accuracy:  82.5\n",
      "Loss after  583  epoch(s):  0.3890239426794067 , Accuracy:  82.5\n",
      "Loss after  584  epoch(s):  0.38976383717644536 , Accuracy:  82.5\n",
      "Loss after  585  epoch(s):  0.3891095241320411 , Accuracy:  82.5\n",
      "Loss after  586  epoch(s):  0.38760973111086494 , Accuracy:  82.5\n",
      "Loss after  587  epoch(s):  0.38673726496823113 , Accuracy:  82.5\n",
      "Loss after  588  epoch(s):  0.38527773189916337 , Accuracy:  83.125\n",
      "Loss after  589  epoch(s):  0.38642760764528883 , Accuracy:  83.125\n",
      "Loss after  590  epoch(s):  0.386034637714852 , Accuracy:  83.125\n",
      "Loss after  591  epoch(s):  0.3857774555594979 , Accuracy:  83.125\n",
      "Loss after  592  epoch(s):  0.3853133294774538 , Accuracy:  83.125\n",
      "Loss after  593  epoch(s):  0.38485678080546 , Accuracy:  83.125\n",
      "Loss after  594  epoch(s):  0.38440560091499576 , Accuracy:  83.125\n",
      "Loss after  595  epoch(s):  0.38411356167842786 , Accuracy:  83.125\n",
      "Loss after  596  epoch(s):  0.38368422155584136 , Accuracy:  83.125\n",
      "Loss after  597  epoch(s):  0.3832572336922918 , Accuracy:  83.125\n",
      "Loss after  598  epoch(s):  0.3856769285983959 , Accuracy:  83.125\n",
      "Loss after  599  epoch(s):  0.38249555711320615 , Accuracy:  83.125\n",
      "Loss after  600  epoch(s):  0.38205490510989826 , Accuracy:  83.125\n",
      "Loss after  601  epoch(s):  0.3816363784919291 , Accuracy:  83.125\n",
      "Loss after  602  epoch(s):  0.38420714793883426 , Accuracy:  83.125\n",
      "Loss after  603  epoch(s):  0.38099006520287704 , Accuracy:  83.125\n",
      "Loss after  604  epoch(s):  0.3805015938420061 , Accuracy:  83.125\n",
      "Loss after  605  epoch(s):  0.38008630782352393 , Accuracy:  83.125\n",
      "Loss after  606  epoch(s):  0.38267229081693 , Accuracy:  83.125\n",
      "Loss after  607  epoch(s):  0.3794504305626096 , Accuracy:  83.125\n",
      "Loss after  608  epoch(s):  0.3789670799497578 , Accuracy:  83.125\n",
      "Loss after  609  epoch(s):  0.37855687100348673 , Accuracy:  83.125\n",
      "Loss after  610  epoch(s):  0.3811552951823713 , Accuracy:  83.125\n",
      "Loss after  611  epoch(s):  0.3779322078835591 , Accuracy:  83.125\n",
      "Loss after  612  epoch(s):  0.37745469865745296 , Accuracy:  83.125\n",
      "Loss after  613  epoch(s):  0.37705026704332534 , Accuracy:  83.125\n",
      "Loss after  614  epoch(s):  0.3796599108987823 , Accuracy:  83.125\n",
      "Loss after  615  epoch(s):  0.3764378873879261 , Accuracy:  83.125\n",
      "Loss after  616  epoch(s):  0.3759662576687502 , Accuracy:  83.125\n",
      "Loss after  617  epoch(s):  0.3755675837176765 , Accuracy:  83.125\n",
      "Loss after  618  epoch(s):  0.3781546027686045 , Accuracy:  83.125\n",
      "Loss after  619  epoch(s):  0.3749927482682904 , Accuracy:  83.125\n",
      "Loss after  620  epoch(s):  0.3745260478385136 , Accuracy:  83.125\n",
      "Loss after  621  epoch(s):  0.3741245972829924 , Accuracy:  83.125\n",
      "Loss after  622  epoch(s):  0.37672140459358955 , Accuracy:  83.125\n",
      "Loss after  623  epoch(s):  0.3735762836987894 , Accuracy:  83.125\n",
      "Loss after  624  epoch(s):  0.3759775972071417 , Accuracy:  83.125\n",
      "Loss after  625  epoch(s):  0.37587739139273557 , Accuracy:  83.125\n",
      "Loss after  626  epoch(s):  0.37257752998579574 , Accuracy:  83.125\n",
      "Loss after  627  epoch(s):  0.3720911688310444 , Accuracy:  83.125\n",
      "Loss after  628  epoch(s):  0.3746790322482788 , Accuracy:  83.125\n",
      "Loss after  629  epoch(s):  0.37142776138326866 , Accuracy:  83.125\n",
      "Loss after  630  epoch(s):  0.3739710415283846 , Accuracy:  83.125\n",
      "Loss after  631  epoch(s):  0.37070322184821036 , Accuracy:  83.125\n",
      "Loss after  632  epoch(s):  0.3732570550915838 , Accuracy:  83.125\n",
      "Loss after  633  epoch(s):  0.37288590775109454 , Accuracy:  83.125\n",
      "Loss after  634  epoch(s):  0.3725644509044242 , Accuracy:  83.125\n",
      "Loss after  635  epoch(s):  0.3722412473086634 , Accuracy:  83.125\n",
      "Loss after  636  epoch(s):  0.3690264907078354 , Accuracy:  83.75\n",
      "Loss after  637  epoch(s):  0.3714515036009697 , Accuracy:  83.125\n",
      "Loss after  638  epoch(s):  0.3711981015335331 , Accuracy:  83.125\n",
      "Loss after  639  epoch(s):  0.3708814574230467 , Accuracy:  83.125\n",
      "Loss after  640  epoch(s):  0.36758349877230617 , Accuracy:  84.375\n",
      "Loss after  641  epoch(s):  0.37003009297785633 , Accuracy:  83.125\n",
      "Loss after  642  epoch(s):  0.3697878468799282 , Accuracy:  83.125\n",
      "Loss after  643  epoch(s):  0.3694756164319196 , Accuracy:  83.125\n",
      "Loss after  644  epoch(s):  0.3691634552068979 , Accuracy:  83.125\n",
      "Loss after  645  epoch(s):  0.36885168402283386 , Accuracy:  83.125\n",
      "Loss after  646  epoch(s):  0.36854029781278996 , Accuracy:  83.75\n",
      "Loss after  647  epoch(s):  0.3651207267245736 , Accuracy:  84.375\n",
      "Loss after  648  epoch(s):  0.3676945740619069 , Accuracy:  83.75\n",
      "Loss after  649  epoch(s):  0.36743605591546596 , Accuracy:  84.375\n",
      "Loss after  650  epoch(s):  0.3669552213746684 , Accuracy:  84.375\n",
      "Loss after  651  epoch(s):  0.36662102873188507 , Accuracy:  84.375\n",
      "Loss after  652  epoch(s):  0.3661825688001385 , Accuracy:  84.375\n",
      "Loss after  653  epoch(s):  0.3659033824749701 , Accuracy:  84.375\n",
      "Loss after  654  epoch(s):  0.36557704633269295 , Accuracy:  84.375\n",
      "Loss after  655  epoch(s):  0.365161428213708 , Accuracy:  84.375\n",
      "Loss after  656  epoch(s):  0.36488258736463025 , Accuracy:  84.375\n",
      "Loss after  657  epoch(s):  0.36456160457667697 , Accuracy:  84.375\n",
      "Loss after  658  epoch(s):  0.36424094973744114 , Accuracy:  84.375\n",
      "Loss after  659  epoch(s):  0.3639211181263794 , Accuracy:  84.375\n",
      "Loss after  660  epoch(s):  0.3634980578990448 , Accuracy:  84.375\n",
      "Loss after  661  epoch(s):  0.36324848230695117 , Accuracy:  84.375\n",
      "Loss after  662  epoch(s):  0.3629310323175907 , Accuracy:  84.375\n",
      "Loss after  663  epoch(s):  0.3626188429133072 , Accuracy:  84.375\n",
      "Loss after  664  epoch(s):  0.362307730944787 , Accuracy:  84.375\n",
      "Loss after  665  epoch(s):  0.3619939308143684 , Accuracy:  84.375\n",
      "Loss after  666  epoch(s):  0.3615907046209994 , Accuracy:  84.375\n",
      "Loss after  667  epoch(s):  0.36131652022560506 , Accuracy:  84.375\n",
      "Loss after  668  epoch(s):  0.3609958527773057 , Accuracy:  84.375\n",
      "Loss after  669  epoch(s):  0.36067606021428117 , Accuracy:  84.375\n",
      "Loss after  670  epoch(s):  0.3603577699120989 , Accuracy:  84.375\n",
      "Loss after  671  epoch(s):  0.36005784883963987 , Accuracy:  84.375\n",
      "Loss after  672  epoch(s):  0.35973553747824355 , Accuracy:  84.375\n",
      "Loss after  673  epoch(s):  0.35932149290831716 , Accuracy:  84.375\n",
      "Loss after  674  epoch(s):  0.35906084250406306 , Accuracy:  84.375\n",
      "Loss after  675  epoch(s):  0.35875442564427495 , Accuracy:  84.375\n",
      "Loss after  676  epoch(s):  0.35846515058356204 , Accuracy:  84.375\n",
      "Loss after  677  epoch(s):  0.3581539956187824 , Accuracy:  84.375\n",
      "Loss after  678  epoch(s):  0.35784924206850144 , Accuracy:  84.375\n",
      "Loss after  679  epoch(s):  0.3575461471766993 , Accuracy:  84.375\n",
      "Loss after  680  epoch(s):  0.35726142661598537 , Accuracy:  84.375\n",
      "Loss after  681  epoch(s):  0.35695458924045603 , Accuracy:  84.375\n",
      "Loss after  682  epoch(s):  0.3566545106854241 , Accuracy:  84.375\n",
      "Loss after  683  epoch(s):  0.3563530447256726 , Accuracy:  84.375\n",
      "Loss after  684  epoch(s):  0.3560702121027982 , Accuracy:  84.375\n",
      "Loss after  685  epoch(s):  0.35576447904879394 , Accuracy:  84.375\n",
      "Loss after  686  epoch(s):  0.355465486190953 , Accuracy:  84.375\n",
      "Loss after  687  epoch(s):  0.3551689037230906 , Accuracy:  84.375\n",
      "Loss after  688  epoch(s):  0.3548920423833847 , Accuracy:  84.375\n",
      "Loss after  689  epoch(s):  0.35459233671886603 , Accuracy:  84.375\n",
      "Loss after  690  epoch(s):  0.35429744397510843 , Accuracy:  84.375\n",
      "Loss after  691  epoch(s):  0.3539251004182846 , Accuracy:  84.375\n",
      "Loss after  692  epoch(s):  0.3534916357918506 , Accuracy:  84.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after  693  epoch(s):  0.3531832665296474 , Accuracy:  84.375\n",
      "Loss after  694  epoch(s):  0.35285399920802807 , Accuracy:  84.375\n",
      "Loss after  695  epoch(s):  0.3524083226495209 , Accuracy:  84.375\n",
      "Loss after  696  epoch(s):  0.3521350583348051 , Accuracy:  84.375\n",
      "Loss after  697  epoch(s):  0.3517933211722078 , Accuracy:  84.375\n",
      "Loss after  698  epoch(s):  0.35145941271188263 , Accuracy:  84.375\n",
      "Loss after  699  epoch(s):  0.35105133373195124 , Accuracy:  84.375\n",
      "Loss after  700  epoch(s):  0.35076926731192143 , Accuracy:  84.375\n",
      "Loss after  701  epoch(s):  0.3504636951397795 , Accuracy:  84.375\n",
      "Loss after  702  epoch(s):  0.3501386420038106 , Accuracy:  84.375\n",
      "Loss after  703  epoch(s):  0.34983800858994285 , Accuracy:  84.375\n",
      "Loss after  704  epoch(s):  0.3494275001337755 , Accuracy:  84.375\n",
      "Loss after  705  epoch(s):  0.34918749670704885 , Accuracy:  84.375\n",
      "Loss after  706  epoch(s):  0.3509009753455036 , Accuracy:  84.375\n",
      "Loss after  707  epoch(s):  0.35063301555509074 , Accuracy:  84.375\n",
      "Loss after  708  epoch(s):  0.35037565287908706 , Accuracy:  84.375\n",
      "Loss after  709  epoch(s):  0.35009981819308217 , Accuracy:  84.375\n",
      "Loss after  710  epoch(s):  0.34983155098030483 , Accuracy:  84.375\n",
      "Loss after  711  epoch(s):  0.3495797935856791 , Accuracy:  84.375\n",
      "Loss after  712  epoch(s):  0.3493089167099583 , Accuracy:  84.375\n",
      "Loss after  713  epoch(s):  0.3490451430731561 , Accuracy:  85.0\n",
      "Loss after  714  epoch(s):  0.34879748109233755 , Accuracy:  85.0\n",
      "Loss after  715  epoch(s):  0.34853071738946967 , Accuracy:  85.0\n",
      "Loss after  716  epoch(s):  0.34828704307536296 , Accuracy:  85.0\n",
      "Loss after  717  epoch(s):  0.34802159485854217 , Accuracy:  85.0\n",
      "Loss after  718  epoch(s):  0.3477627572233713 , Accuracy:  85.0\n",
      "Loss after  719  epoch(s):  0.34752233912370323 , Accuracy:  85.0\n",
      "Loss after  720  epoch(s):  0.3472606823548978 , Accuracy:  85.0\n",
      "Loss after  721  epoch(s):  0.34700569879233667 , Accuracy:  85.0\n",
      "Loss after  722  epoch(s):  0.344821452636671 , Accuracy:  85.625\n",
      "Loss after  723  epoch(s):  0.3464924211168259 , Accuracy:  85.0\n",
      "Loss after  724  epoch(s):  0.3462598897522252 , Accuracy:  85.0\n",
      "Loss after  725  epoch(s):  0.3460016623860716 , Accuracy:  85.0\n",
      "Loss after  726  epoch(s):  0.34575091620656834 , Accuracy:  85.625\n",
      "Loss after  727  epoch(s):  0.3455171240201239 , Accuracy:  85.625\n",
      "Loss after  728  epoch(s):  0.34526416099085755 , Accuracy:  85.625\n",
      "Loss after  729  epoch(s):  0.3450337566040608 , Accuracy:  85.625\n",
      "Loss after  730  epoch(s):  0.34478262760454503 , Accuracy:  85.625\n",
      "Loss after  731  epoch(s):  0.34455545725986764 , Accuracy:  85.625\n",
      "Loss after  732  epoch(s):  0.34430604896248623 , Accuracy:  85.625\n",
      "Loss after  733  epoch(s):  0.3440634486518479 , Accuracy:  85.625\n",
      "Loss after  734  epoch(s):  0.34383921952444607 , Accuracy:  85.625\n",
      "Loss after  735  epoch(s):  0.3435941318977179 , Accuracy:  85.625\n",
      "Loss after  736  epoch(s):  0.3414717841752414 , Accuracy:  85.625\n",
      "Loss after  737  epoch(s):  0.3431120497565873 , Accuracy:  85.625\n",
      "Loss after  738  epoch(s):  0.34289479883374147 , Accuracy:  85.625\n",
      "Loss after  739  epoch(s):  0.3426510621449003 , Accuracy:  85.625\n",
      "Loss after  740  epoch(s):  0.34243136404634955 , Accuracy:  85.625\n",
      "Loss after  741  epoch(s):  0.3421903432952272 , Accuracy:  85.625\n",
      "Loss after  742  epoch(s):  0.3419565753869125 , Accuracy:  85.625\n",
      "Loss after  743  epoch(s):  0.341740382747293 , Accuracy:  85.625\n",
      "Loss after  744  epoch(s):  0.3415051957273753 , Accuracy:  86.25\n",
      "Loss after  745  epoch(s):  0.34129273141552274 , Accuracy:  86.25\n",
      "Loss after  746  epoch(s):  0.3410591827629408 , Accuracy:  86.25\n",
      "Loss after  747  epoch(s):  0.34084915318710574 , Accuracy:  86.25\n",
      "Loss after  748  epoch(s):  0.3406174052458442 , Accuracy:  86.25\n",
      "Loss after  749  epoch(s):  0.34040952477966285 , Accuracy:  86.25\n",
      "Loss after  750  epoch(s):  0.33832653747066477 , Accuracy:  86.25\n",
      "Loss after  751  epoch(s):  0.33995692398031974 , Accuracy:  86.25\n",
      "Loss after  752  epoch(s):  0.33973179887072325 , Accuracy:  86.25\n",
      "Loss after  753  epoch(s):  0.339525615374356 , Accuracy:  86.25\n",
      "Loss after  754  epoch(s):  0.3392982947340447 , Accuracy:  86.25\n",
      "Loss after  755  epoch(s):  0.3390952417924945 , Accuracy:  86.25\n",
      "Loss after  756  epoch(s):  0.33887046250631786 , Accuracy:  86.25\n",
      "Loss after  757  epoch(s):  0.33866986895496193 , Accuracy:  86.25\n",
      "Loss after  758  epoch(s):  0.3384473429940538 , Accuracy:  86.25\n",
      "Loss after  759  epoch(s):  0.33832099499107127 , Accuracy:  86.25\n",
      "Loss after  760  epoch(s):  0.33810668163464597 , Accuracy:  86.25\n",
      "Loss after  761  epoch(s):  0.3378441508284114 , Accuracy:  86.25\n",
      "Loss after  762  epoch(s):  0.3376953220598339 , Accuracy:  86.25\n",
      "Loss after  763  epoch(s):  0.3356930143957652 , Accuracy:  86.25\n",
      "Loss after  764  epoch(s):  0.3372073295691022 , Accuracy:  86.25\n",
      "Loss after  765  epoch(s):  0.3370147366831936 , Accuracy:  86.25\n",
      "Loss after  766  epoch(s):  0.33686651733991513 , Accuracy:  86.25\n",
      "Loss after  767  epoch(s):  0.3366088905237521 , Accuracy:  86.25\n",
      "Loss after  768  epoch(s):  0.33646288723336015 , Accuracy:  86.25\n",
      "Loss after  769  epoch(s):  0.33627670158713213 , Accuracy:  86.25\n",
      "Loss after  770  epoch(s):  0.3360694775640921 , Accuracy:  86.25\n",
      "Loss after  771  epoch(s):  0.3358842662619614 , Accuracy:  86.25\n",
      "Loss after  772  epoch(s):  0.33569635107916856 , Accuracy:  86.25\n",
      "Loss after  773  epoch(s):  0.33548930217648426 , Accuracy:  86.25\n",
      "Loss after  774  epoch(s):  0.33530651405201234 , Accuracy:  86.25\n",
      "Loss after  775  epoch(s):  0.3351013365679907 , Accuracy:  86.25\n",
      "Loss after  776  epoch(s):  0.3331457400876653 , Accuracy:  86.25\n",
      "Loss after  777  epoch(s):  0.33463536961504065 , Accuracy:  86.25\n",
      "Loss after  778  epoch(s):  0.33451846768493987 , Accuracy:  86.25\n",
      "Loss after  779  epoch(s):  0.33433459197058235 , Accuracy:  86.25\n",
      "Loss after  780  epoch(s):  0.334131756579911 , Accuracy:  86.25\n",
      "Loss after  781  epoch(s):  0.333953482150922 , Accuracy:  86.25\n",
      "Loss after  782  epoch(s):  0.3337531904977628 , Accuracy:  86.25\n",
      "Loss after  783  epoch(s):  0.3335767995240777 , Accuracy:  86.25\n",
      "Loss after  784  epoch(s):  0.33337872362574233 , Accuracy:  86.25\n",
      "Loss after  785  epoch(s):  0.33320397150075826 , Accuracy:  86.25\n",
      "Loss after  786  epoch(s):  0.3330264029420533 , Accuracy:  86.25\n",
      "Loss after  787  epoch(s):  0.33282974888773753 , Accuracy:  86.25\n",
      "Loss after  788  epoch(s):  0.3309199673229768 , Accuracy:  86.25\n",
      "Loss after  789  epoch(s):  0.3324492885652489 , Accuracy:  86.25\n",
      "Loss after  790  epoch(s):  0.33228059240742164 , Accuracy:  86.25\n",
      "Loss after  791  epoch(s):  0.3321049789967997 , Accuracy:  86.25\n",
      "Loss after  792  epoch(s):  0.33191092726858357 , Accuracy:  86.25\n",
      "Loss after  793  epoch(s):  0.33174127717418606 , Accuracy:  86.25\n",
      "Loss after  794  epoch(s):  0.3315501242139138 , Accuracy:  86.25\n",
      "Loss after  795  epoch(s):  0.33138244853721766 , Accuracy:  86.25\n",
      "Loss after  796  epoch(s):  0.33121207871068775 , Accuracy:  86.25\n",
      "Loss after  797  epoch(s):  0.3310229410346451 , Accuracy:  86.25\n",
      "Loss after  798  epoch(s):  0.33085795426139575 , Accuracy:  86.25\n",
      "Loss after  799  epoch(s):  0.32898613966894297 , Accuracy:  86.25\n",
      "Loss after  800  epoch(s):  0.3304903480498344 , Accuracy:  86.25\n",
      "Loss after  801  epoch(s):  0.33032984106932634 , Accuracy:  86.25\n",
      "Loss after  802  epoch(s):  0.3301623473252596 , Accuracy:  86.25\n",
      "Loss after  803  epoch(s):  0.3299761441685024 , Accuracy:  86.25\n",
      "Loss after  804  epoch(s):  0.3298149287302675 , Accuracy:  86.25\n",
      "Loss after  805  epoch(s):  0.32963179754153427 , Accuracy:  86.25\n",
      "Loss after  806  epoch(s):  0.3294725384612841 , Accuracy:  86.25\n",
      "Loss after  807  epoch(s):  0.329310451468636 , Accuracy:  86.25\n",
      "Loss after  808  epoch(s):  0.32912953767910597 , Accuracy:  86.25\n",
      "Loss after  809  epoch(s):  0.3272982471374148 , Accuracy:  86.25\n",
      "Loss after  810  epoch(s):  0.3288011163454601 , Accuracy:  86.25\n",
      "Loss after  811  epoch(s):  0.3286244575835088 , Accuracy:  86.25\n",
      "Loss after  812  epoch(s):  0.3284680060939692 , Accuracy:  86.25\n",
      "Loss after  813  epoch(s):  0.3283090122247817 , Accuracy:  86.25\n",
      "Loss after  814  epoch(s):  0.3281315350569699 , Accuracy:  86.25\n",
      "Loss after  815  epoch(s):  0.32797838247299743 , Accuracy:  86.25\n",
      "Loss after  816  epoch(s):  0.32782235700795564 , Accuracy:  86.25\n",
      "Loss after  817  epoch(s):  0.32764758490093204 , Accuracy:  86.25\n",
      "Loss after  818  epoch(s):  0.327497065549855 , Accuracy:  86.25\n",
      "Loss after  819  epoch(s):  0.3256991123221149 , Accuracy:  86.25\n",
      "Loss after  820  epoch(s):  0.32716018860777984 , Accuracy:  86.25\n",
      "Loss after  821  epoch(s):  0.3270113713159459 , Accuracy:  86.25\n",
      "Loss after  822  epoch(s):  0.32685813866134755 , Accuracy:  86.25\n",
      "Loss after  823  epoch(s):  0.3266868643908992 , Accuracy:  86.25\n",
      "Loss after  824  epoch(s):  0.3265396154522285 , Accuracy:  86.25\n",
      "Loss after  825  epoch(s):  0.3263897093703131 , Accuracy:  86.25\n",
      "Loss after  826  epoch(s):  0.32624026153921737 , Accuracy:  86.25\n",
      "Loss after  827  epoch(s):  0.32607217736962246 , Accuracy:  86.25\n",
      "Loss after  828  epoch(s):  0.32592846523877755 , Accuracy:  86.25\n",
      "Loss after  829  epoch(s):  0.3241678867496184 , Accuracy:  86.25\n",
      "Loss after  830  epoch(s):  0.3256060171402244 , Accuracy:  86.25\n",
      "Loss after  831  epoch(s):  0.3254662316440603 , Accuracy:  86.25\n",
      "Loss after  832  epoch(s):  0.3253199395653047 , Accuracy:  86.25\n",
      "Loss after  833  epoch(s):  0.3251743467513876 , Accuracy:  86.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after  834  epoch(s):  0.3250104175883714 , Accuracy:  86.25\n",
      "Loss after  835  epoch(s):  0.32487084196046323 , Accuracy:  86.25\n",
      "Loss after  836  epoch(s):  0.3247283515603674 , Accuracy:  86.25\n",
      "Loss after  837  epoch(s):  0.32456734570097157 , Accuracy:  86.25\n",
      "Loss after  838  epoch(s):  0.32284215409040584 , Accuracy:  86.25\n",
      "Loss after  839  epoch(s):  0.3242806090828231 , Accuracy:  86.25\n",
      "Loss after  840  epoch(s):  0.3241423764291177 , Accuracy:  86.25\n",
      "Loss after  841  epoch(s):  0.3239822176178187 , Accuracy:  86.25\n",
      "Loss after  842  epoch(s):  0.32384646511920856 , Accuracy:  86.25\n",
      "Loss after  843  epoch(s):  0.32370792758083217 , Accuracy:  86.25\n",
      "Loss after  844  epoch(s):  0.32356989259725516 , Accuracy:  86.25\n",
      "Loss after  845  epoch(s):  0.32341351282558534 , Accuracy:  86.25\n",
      "Loss after  846  epoch(s):  0.3217149592731291 , Accuracy:  86.25\n",
      "Loss after  847  epoch(s):  0.323136713056326 , Accuracy:  86.25\n",
      "Loss after  848  epoch(s):  0.32300315288931497 , Accuracy:  86.25\n",
      "Loss after  849  epoch(s):  0.32284788513620055 , Accuracy:  86.25\n",
      "Loss after  850  epoch(s):  0.32271687614653766 , Accuracy:  86.25\n",
      "Loss after  851  epoch(s):  0.3225831206116393 , Accuracy:  86.25\n",
      "Loss after  852  epoch(s):  0.3224498656057778 , Accuracy:  86.25\n",
      "Loss after  853  epoch(s):  0.3222983919471113 , Accuracy:  86.25\n",
      "Loss after  854  epoch(s):  0.32217073965594817 , Accuracy:  86.25\n",
      "Loss after  855  epoch(s):  0.3205000940918734 , Accuracy:  86.25\n",
      "Loss after  856  epoch(s):  0.32190127195003215 , Accuracy:  86.25\n",
      "Loss after  857  epoch(s):  0.32175404935185625 , Accuracy:  86.25\n",
      "Loss after  858  epoch(s):  0.3216273252479058 , Accuracy:  86.25\n",
      "Loss after  859  epoch(s):  0.3214979167165738 , Accuracy:  86.25\n",
      "Loss after  860  epoch(s):  0.32136906976431645 , Accuracy:  86.25\n",
      "Loss after  861  epoch(s):  0.3212221534007006 , Accuracy:  86.25\n",
      "Loss after  862  epoch(s):  0.3210990085831121 , Accuracy:  86.25\n",
      "Loss after  863  epoch(s):  0.3194537672628571 , Accuracy:  86.25\n",
      "Loss after  864  epoch(s):  0.32083903168235917 , Accuracy:  86.25\n",
      "Loss after  865  epoch(s):  0.32071490962327065 , Accuracy:  86.25\n",
      "Loss after  866  epoch(s):  0.32056953135025157 , Accuracy:  86.25\n",
      "Loss after  867  epoch(s):  0.3204480702363457 , Accuracy:  86.25\n",
      "Loss after  868  epoch(s):  0.32032389441233305 , Accuracy:  86.25\n",
      "Loss after  869  epoch(s):  0.3202002041809409 , Accuracy:  86.25\n",
      "Loss after  870  epoch(s):  0.32005851924387746 , Accuracy:  86.25\n",
      "Loss after  871  epoch(s):  0.3199403832218282 , Accuracy:  86.25\n",
      "Loss after  872  epoch(s):  0.31832516073970835 , Accuracy:  86.25\n",
      "Loss after  873  epoch(s):  0.3196908142536613 , Accuracy:  86.25\n",
      "Loss after  874  epoch(s):  0.3195715482840658 , Accuracy:  86.25\n",
      "Loss after  875  epoch(s):  0.3194496046042743 , Accuracy:  86.25\n",
      "Loss after  876  epoch(s):  0.31931011046118296 , Accuracy:  86.25\n",
      "Loss after  877  epoch(s):  0.3191941474018789 , Accuracy:  86.25\n",
      "Loss after  878  epoch(s):  0.3190754740138888 , Accuracy:  86.25\n",
      "Loss after  879  epoch(s):  0.31748054255092717 , Accuracy:  86.25\n",
      "Loss after  880  epoch(s):  0.31883227865484703 , Accuracy:  86.25\n",
      "Loss after  881  epoch(s):  0.31869784293110104 , Accuracy:  86.25\n",
      "Loss after  882  epoch(s):  0.3185836079502538 , Accuracy:  86.25\n",
      "Loss after  883  epoch(s):  0.3184668195670543 , Accuracy:  86.25\n",
      "Loss after  884  epoch(s):  0.31835059155071277 , Accuracy:  86.25\n",
      "Loss after  885  epoch(s):  0.3182349056390364 , Accuracy:  86.25\n",
      "Loss after  886  epoch(s):  0.3181015315787859 , Accuracy:  86.25\n",
      "Loss after  887  epoch(s):  0.31799130414845433 , Accuracy:  86.25\n",
      "Loss after  888  epoch(s):  0.31642570777491885 , Accuracy:  86.25\n",
      "Loss after  889  epoch(s):  0.3177586464466974 , Accuracy:  86.25\n",
      "Loss after  890  epoch(s):  0.31764730638207556 , Accuracy:  86.25\n",
      "Loss after  891  epoch(s):  0.31753340024557675 , Accuracy:  86.25\n",
      "Loss after  892  epoch(s):  0.3174021644756143 , Accuracy:  86.25\n",
      "Loss after  893  epoch(s):  0.31729413370506787 , Accuracy:  86.25\n",
      "Loss after  894  epoch(s):  0.3171833713424654 , Accuracy:  86.25\n",
      "Loss after  895  epoch(s):  0.31707299562123936 , Accuracy:  86.25\n",
      "Loss after  896  epoch(s):  0.31553126880893967 , Accuracy:  86.25\n",
      "Loss after  897  epoch(s):  0.31684714745078263 , Accuracy:  86.25\n",
      "Loss after  898  epoch(s):  0.31672113124640067 , Accuracy:  86.25\n",
      "Loss after  899  epoch(s):  0.31661511472773335 , Accuracy:  86.25\n",
      "Loss after  900  epoch(s):  0.3165065599162057 , Accuracy:  86.25\n",
      "Loss after  901  epoch(s):  0.31639853015088903 , Accuracy:  86.25\n",
      "Loss after  902  epoch(s):  0.31629100665463705 , Accuracy:  86.25\n",
      "Loss after  903  epoch(s):  0.314768513064582 , Accuracy:  86.25\n",
      "Loss after  904  epoch(s):  0.31607127131849433 , Accuracy:  86.25\n",
      "Loss after  905  epoch(s):  0.3159484539373415 , Accuracy:  86.25\n",
      "Loss after  906  epoch(s):  0.3158454343194289 , Accuracy:  86.25\n",
      "Loss after  907  epoch(s):  0.315739922191372 , Accuracy:  86.25\n",
      "Loss after  908  epoch(s):  0.3156349455211564 , Accuracy:  86.25\n",
      "Loss after  909  epoch(s):  0.31553048001294165 , Accuracy:  86.25\n",
      "Loss after  910  epoch(s):  0.3155205202248657 , Accuracy:  86.25\n",
      "Loss after  911  epoch(s):  0.31538697927676873 , Accuracy:  86.25\n",
      "Loss after  912  epoch(s):  0.31527514176224236 , Accuracy:  86.25\n",
      "Loss after  913  epoch(s):  0.3151653593570241 , Accuracy:  86.25\n",
      "Loss after  914  epoch(s):  0.3136610224896592 , Accuracy:  86.25\n",
      "Loss after  915  epoch(s):  0.3149450383280807 , Accuracy:  86.25\n",
      "Loss after  916  epoch(s):  0.3148404782924969 , Accuracy:  86.25\n",
      "Loss after  917  epoch(s):  0.3147341824339911 , Accuracy:  86.25\n",
      "Loss after  918  epoch(s):  0.314629171155216 , Accuracy:  86.25\n",
      "Loss after  919  epoch(s):  0.3145252615226427 , Accuracy:  86.25\n",
      "Loss after  920  epoch(s):  0.31440465559666825 , Accuracy:  86.25\n",
      "Loss after  921  epoch(s):  0.3129316574680535 , Accuracy:  86.25\n",
      "Loss after  922  epoch(s):  0.3142012316592668 , Accuracy:  86.25\n",
      "Loss after  923  epoch(s):  0.3141025019606075 , Accuracy:  86.25\n",
      "Loss after  924  epoch(s):  0.31400155126093643 , Accuracy:  86.25\n",
      "Loss after  925  epoch(s):  0.3139014607519107 , Accuracy:  86.25\n",
      "Loss after  926  epoch(s):  0.3138021381914967 , Accuracy:  86.25\n",
      "Loss after  927  epoch(s):  0.313795324496018 , Accuracy:  86.25\n",
      "Loss after  928  epoch(s):  0.3136680155325361 , Accuracy:  86.25\n",
      "Loss after  929  epoch(s):  0.31356197073985115 , Accuracy:  86.25\n",
      "Loss after  930  epoch(s):  0.3120956200860331 , Accuracy:  86.25\n",
      "Loss after  931  epoch(s):  0.313351012485638 , Accuracy:  86.25\n",
      "Loss after  932  epoch(s):  0.31324862892206695 , Accuracy:  86.25\n",
      "Loss after  933  epoch(s):  0.31314946628593743 , Accuracy:  86.25\n",
      "Loss after  934  epoch(s):  0.3130517855506812 , Accuracy:  86.25\n",
      "Loss after  935  epoch(s):  0.3116084750709261 , Accuracy:  86.25\n",
      "Loss after  936  epoch(s):  0.3128554324928253 , Accuracy:  86.25\n",
      "Loss after  937  epoch(s):  0.3127619817962421 , Accuracy:  86.25\n",
      "Loss after  938  epoch(s):  0.31266670130950114 , Accuracy:  86.25\n",
      "Loss after  939  epoch(s):  0.3125725145938277 , Accuracy:  86.25\n",
      "Loss after  940  epoch(s):  0.31246187714221463 , Accuracy:  86.25\n",
      "Loss after  941  epoch(s):  0.3124809859982095 , Accuracy:  86.25\n",
      "Loss after  942  epoch(s):  0.3123574061537793 , Accuracy:  86.25\n",
      "Loss after  943  epoch(s):  0.3109223157012664 , Accuracy:  86.25\n",
      "Loss after  944  epoch(s):  0.31215608958527347 , Accuracy:  86.25\n",
      "Loss after  945  epoch(s):  0.3120612697203877 , Accuracy:  86.25\n",
      "Loss after  946  epoch(s):  0.31196533351767763 , Accuracy:  86.25\n",
      "Loss after  947  epoch(s):  0.3118710470234687 , Accuracy:  86.25\n",
      "Loss after  948  epoch(s):  0.3104597613605923 , Accuracy:  86.25\n",
      "Loss after  949  epoch(s):  0.3116822836817996 , Accuracy:  86.25\n",
      "Loss after  950  epoch(s):  0.3115925299059322 , Accuracy:  86.25\n",
      "Loss after  951  epoch(s):  0.31150108389184383 , Accuracy:  86.25\n",
      "Loss after  952  epoch(s):  0.3101013332719883 , Accuracy:  86.25\n",
      "Loss after  953  epoch(s):  0.3113174183357911 , Accuracy:  86.25\n",
      "Loss after  954  epoch(s):  0.31122990259823297 , Accuracy:  86.25\n",
      "Loss after  955  epoch(s):  0.31122888518136066 , Accuracy:  86.25\n",
      "Loss after  956  epoch(s):  0.3111126141742385 , Accuracy:  86.25\n",
      "Loss after  957  epoch(s):  0.3110169182425032 , Accuracy:  86.25\n",
      "Loss after  958  epoch(s):  0.31092353290442654 , Accuracy:  86.25\n",
      "Loss after  959  epoch(s):  0.3108320284761031 , Accuracy:  86.25\n",
      "Loss after  960  epoch(s):  0.3094474952534745 , Accuracy:  86.25\n",
      "Loss after  961  epoch(s):  0.3106497431991811 , Accuracy:  86.25\n",
      "Loss after  962  epoch(s):  0.31056315384259536 , Accuracy:  86.25\n",
      "Loss after  963  epoch(s):  0.3104750080211456 , Accuracy:  86.25\n",
      "Loss after  964  epoch(s):  0.3091027347752125 , Accuracy:  86.25\n",
      "Loss after  965  epoch(s):  0.31029850637891865 , Accuracy:  86.25\n",
      "Loss after  966  epoch(s):  0.31021440355831853 , Accuracy:  86.25\n",
      "Loss after  967  epoch(s):  0.31023248355702665 , Accuracy:  86.25\n",
      "Loss after  968  epoch(s):  0.3101160169791587 , Accuracy:  86.25\n",
      "Loss after  969  epoch(s):  0.3100229961065922 , Accuracy:  86.25\n",
      "Loss after  970  epoch(s):  0.30993246595279966 , Accuracy:  86.25\n",
      "Loss after  971  epoch(s):  0.30857014264769916 , Accuracy:  86.25\n",
      "Loss after  972  epoch(s):  0.30975382613116736 , Accuracy:  86.25\n",
      "Loss after  973  epoch(s):  0.30966928189779896 , Accuracy:  86.25\n",
      "Loss after  974  epoch(s):  0.30958343173523745 , Accuracy:  86.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after  975  epoch(s):  0.30823518802039274 , Accuracy:  86.25\n",
      "Loss after  976  epoch(s):  0.3094122760923936 , Accuracy:  86.25\n",
      "Loss after  977  epoch(s):  0.30933081369641685 , Accuracy:  86.25\n",
      "Loss after  978  epoch(s):  0.30933305893750773 , Accuracy:  86.25\n",
      "Loss after  979  epoch(s):  0.30922396088679954 , Accuracy:  86.25\n",
      "Loss after  980  epoch(s):  0.30913489689399015 , Accuracy:  86.25\n",
      "Loss after  981  epoch(s):  0.30904825435216415 , Accuracy:  86.25\n",
      "Loss after  982  epoch(s):  0.3089634682730365 , Accuracy:  86.25\n",
      "Loss after  983  epoch(s):  0.3076314793260573 , Accuracy:  86.25\n",
      "Loss after  984  epoch(s):  0.3087950900221496 , Accuracy:  86.25\n",
      "Loss after  985  epoch(s):  0.3087150688504322 , Accuracy:  86.25\n",
      "Loss after  986  epoch(s):  0.3086335797878833 , Accuracy:  86.25\n",
      "Loss after  987  epoch(s):  0.30865430010700057 , Accuracy:  86.25\n",
      "Loss after  988  epoch(s):  0.30854417016841607 , Accuracy:  86.25\n",
      "Loss after  989  epoch(s):  0.3072134913004444 , Accuracy:  86.25\n",
      "Loss after  990  epoch(s):  0.308369191560503 , Accuracy:  86.25\n",
      "Loss after  991  epoch(s):  0.30828714217414943 , Accuracy:  86.25\n",
      "Loss after  992  epoch(s):  0.3082042775139654 , Accuracy:  86.25\n",
      "Loss after  993  epoch(s):  0.30812309052490494 , Accuracy:  86.25\n",
      "Loss after  994  epoch(s):  0.30681657473959667 , Accuracy:  86.25\n",
      "Loss after  995  epoch(s):  0.3079618744526819 , Accuracy:  86.25\n",
      "Loss after  996  epoch(s):  0.30798517983980533 , Accuracy:  86.25\n",
      "Loss after  997  epoch(s):  0.3078765200733389 , Accuracy:  86.25\n",
      "Loss after  998  epoch(s):  0.3077906077191716 , Accuracy:  86.25\n",
      "Loss after  999  epoch(s):  0.3077074107976765 , Accuracy:  86.25\n",
      "Final loss:  0.30640719758539153 Final accuracy:  86.25\n",
      "6.948704957962036\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn = NeuralNetwork(structure)\n",
    "from time import time\n",
    "\n",
    "tic = time()\n",
    "nn.train(X, Y, n_epochs=1000, batch_size=8, loss_func = \"binary_cross_entropy\")\n",
    "print(time() - tic)\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(0, 9, 0.1), np.arange(0, 9, 0.1))\n",
    "\n",
    "X_vis = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "h = nn.predict(X_vis)\n",
    "h = np.array(h) >= 0.5\n",
    "h = np.reshape(h, (len(xx), len(yy)))\n",
    "\n",
    "idx0 = [i for i, v in enumerate(Y) if v == 0]\n",
    "idx1 = [i for i, v in enumerate(Y) if v == 1]\n",
    "\n",
    "plt.contourf(xx, yy, h, cmap='jet')\n",
    "plt.scatter(X[idx1, 0], X[idx1, 1], marker='^', c=\"red\", edgecolors=\"white\", label=\"class 1\")\n",
    "plt.scatter(X[idx0, 0], X[idx0, 1], marker='o', c=\"blue\", edgecolors=\"white\", label=\"class 0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "64b57133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.8098427652515948\n",
      "Loss:  0.7897093893051735\n",
      "Loss:  0.770968734764202\n",
      "Loss:  0.7536647626159485\n",
      "Loss:  0.7379336406797041\n",
      "Loss:  0.7233068122165249\n",
      "Loss:  0.7097481077455765\n",
      "Loss:  0.697177426482199\n",
      "Loss:  0.6856613802043514\n",
      "Loss:  0.675191400963587\n",
      "Loss:  0.6656247430161741\n",
      "Loss:  0.6568166328921515\n",
      "Loss:  0.6487141652041147\n",
      "Loss:  0.6412666711835093\n",
      "Loss:  0.6344658776207502\n",
      "Loss:  0.6282630624046945\n",
      "Loss:  0.6225683955671591\n",
      "Loss:  0.6173422972556197\n",
      "Loss:  0.612547746027818\n",
      "Loss:  0.608181644121121\n",
      "Loss:  0.6042110442183192\n",
      "Loss:  0.6005679600203121\n",
      "Loss:  0.5972507013395898\n",
      "Loss:  0.5942187378301567\n",
      "Loss:  0.5914190668113865\n",
      "Loss:  0.5888128012977788\n",
      "Loss:  0.5862986209988881\n",
      "Loss:  0.5839873463843052\n",
      "Loss:  0.5818622766201124\n",
      "Loss:  0.5799080462760194\n",
      "Loss:  0.5781218924302646\n",
      "Loss:  0.5765073025380966\n",
      "Loss:  0.5750307829157509\n",
      "Loss:  0.5736943873848649\n",
      "Loss:  0.5724627077293551\n",
      "Loss:  0.5713269205724797\n",
      "Loss:  0.5702902868564343\n",
      "Loss:  0.5693463977152149\n",
      "Loss:  0.5684747572178015\n",
      "Loss:  0.567669486311468\n",
      "Loss:  0.5669251877128831\n",
      "Loss:  0.5662369067297656\n",
      "Loss:  0.5656000951234219\n",
      "Loss:  0.565010577811839\n",
      "Loss:  0.5644645222183841\n",
      "Loss:  0.5639584100782923\n",
      "Loss:  0.5634890115234686\n",
      "Loss:  0.5630533612752447\n",
      "Loss:  0.5626487367842946\n",
      "Loss:  0.5622726381666473\n",
      "Loss:  0.5619227697944442\n",
      "Loss:  0.5615970234096239\n",
      "Loss:  0.5612934626379611\n",
      "Loss:  0.561010308789767\n",
      "Loss:  0.560745927842021\n",
      "Loss:  0.5604988185047073\n",
      "Loss:  0.5602676012816821\n",
      "Loss:  0.5600510084434525\n",
      "Loss:  0.5598478748358605\n",
      "Loss:  0.5596571294548\n",
      "Loss:  0.5594777877227999\n",
      "Loss:  0.5593089444085834\n",
      "Loss:  0.5591497671355921\n",
      "Loss:  0.5589994904299622\n",
      "Loss:  0.5588574102625852\n",
      "Loss:  0.5587228790436943\n",
      "Loss:  0.5585953010319262\n",
      "Loss:  0.5584741281230154\n",
      "Loss:  0.5583588559862341\n",
      "Loss:  0.5582490205193892\n",
      "Loss:  0.5581441945956679\n",
      "Loss:  0.5580439850778864\n",
      "Loss:  0.5579476096667514\n",
      "Loss:  0.5578422294973671\n",
      "Loss:  0.5577411535978043\n",
      "Loss:  0.5576440470456323\n",
      "Loss:  0.5575506013214211\n",
      "Loss:  0.5574605322010246\n",
      "Loss:  0.5573735778187647\n",
      "Loss:  0.5572894968874045\n",
      "Loss:  0.557208067061989\n",
      "Loss:  0.5571290834357163\n",
      "Loss:  0.5570523571569937\n",
      "Loss:  0.5569777141577406\n",
      "Loss:  0.556904993983834\n",
      "Loss:  0.5568340487193513\n",
      "Loss:  0.5567647419969551\n",
      "Loss:  0.5566969480874122\n",
      "Loss:  0.5566305510618095\n",
      "Loss:  0.5565654440205703\n",
      "Loss:  0.55650152838386\n",
      "Loss:  0.5564387132384168\n",
      "Loss:  0.5563769147362495\n",
      "Loss:  0.5563160555410279\n",
      "Loss:  0.5562560643183213\n",
      "Loss:  0.5561968752661663\n",
      "Loss:  0.5561384276827293\n",
      "Loss:  0.5560806655680895\n",
      "Loss:  0.5560235372574187\n",
      "Loss:  0.5559669950830438\n",
      "Loss:  0.5559109950630967\n",
      "Loss:  0.5558554966146295\n",
      "Loss:  0.5558004622892501\n",
      "Loss:  0.5557458575294919\n",
      "Loss:  0.555691650444272\n",
      "Loss:  0.5556378116019255\n",
      "Loss:  0.5555843138394259\n",
      "Loss:  0.5555311320865137\n",
      "Loss:  0.5554782432035559\n",
      "Loss:  0.5554256258320527\n",
      "Loss:  0.5553732602567978\n",
      "Loss:  0.555321128278775\n",
      "Loss:  0.5552692130979455\n",
      "Loss:  0.5552174992051541\n",
      "Loss:  0.5551659722824327\n",
      "Loss:  0.5551146191110504\n",
      "Loss:  0.5550634274866981\n",
      "Loss:  0.5550123861412525\n",
      "Loss:  0.5549614846706049\n",
      "Loss:  0.554910713468083\n",
      "Loss:  0.5548600636630241\n",
      "Loss:  0.5548095270641058\n",
      "Loss:  0.5547590961070578\n",
      "Loss:  0.5547087638064159\n",
      "Loss:  0.5546585237110044\n",
      "Loss:  0.5546083698628562\n",
      "Loss:  0.5545582967593043\n",
      "Loss:  0.5545082993179979\n",
      "Loss:  0.5544583728446169\n",
      "Loss:  0.5544096698555844\n",
      "Loss:  0.5543627214732634\n",
      "Loss:  0.5543158000668307\n",
      "Loss:  0.5542689044111058\n",
      "Loss:  0.5542220333734964\n",
      "Loss:  0.5541751859069567\n",
      "Loss:  0.5541283610434794\n",
      "Loss:  0.5540815578880867\n",
      "Loss:  0.5540347756132772\n",
      "Loss:  0.5539880134538961\n",
      "Loss:  0.5539412707023967\n",
      "Loss:  0.5538945467044615\n",
      "Loss:  0.553847840854958\n",
      "Loss:  0.5538011525942007\n",
      "Loss:  0.5537544814045001\n",
      "Loss:  0.553707826806971\n",
      "Loss:  0.5536611883585862\n",
      "Loss:  0.5536145656494534\n",
      "Loss:  0.553567958300299\n",
      "Loss:  0.5535213659601433\n",
      "Loss:  0.5534747883041529\n",
      "Loss:  0.5534282250316563\n",
      "Loss:  0.5533816758643099\n",
      "Loss:  0.553335140544403\n",
      "Loss:  0.5532886188332936\n",
      "Loss:  0.5532421105099606\n",
      "Loss:  0.5531956153696673\n",
      "Loss:  0.5531491332227267\n",
      "Loss:  0.5531026638933596\n",
      "Loss:  0.55305620721864\n",
      "Loss:  0.5530097630475213\n",
      "Loss:  0.5529633312399342\n",
      "Loss:  0.5529169116659564\n",
      "Loss:  0.5528705042050417\n",
      "Loss:  0.5528241087453111\n",
      "Loss:  0.5527777251828949\n",
      "Loss:  0.5527313534213265\n",
      "Loss:  0.5526849933709813\n",
      "Loss:  0.5526386449485585\n",
      "Loss:  0.5525923080766024\n",
      "Loss:  0.5525459826830599\n",
      "Loss:  0.552499668700871\n",
      "Loss:  0.552453366067591\n",
      "Loss:  0.552407074725042\n",
      "Loss:  0.5523607946189891\n",
      "Loss:  0.5523145256988422\n",
      "Loss:  0.5522682679173812\n",
      "Loss:  0.5522220212304998\n",
      "Loss:  0.5521757855969712\n",
      "Loss:  0.5521295609782302\n",
      "Loss:  0.5520833473381715\n",
      "Loss:  0.5520371446429647\n",
      "Loss:  0.551990952860882\n",
      "Loss:  0.5519447719621388\n",
      "Loss:  0.5518986019187491\n",
      "Loss:  0.5518524427043874\n",
      "Loss:  0.5518062942942643\n",
      "Loss:  0.5517601566650118\n",
      "Loss:  0.5517140297945742\n",
      "Loss:  0.5516679136621105\n",
      "Loss:  0.5516218082479029\n",
      "Loss:  0.5515757135332716\n",
      "Loss:  0.5515296295004972\n",
      "Loss:  0.5514835561327482\n",
      "Loss:  0.5514374934140145\n",
      "Loss:  0.5513914413290454\n",
      "Loss:  0.5513453998632931\n",
      "Loss:  0.5512993690028593\n",
      "Loss:  0.5512533487344469\n",
      "Loss:  0.5512073390453149\n",
      "Loss:  0.5511613399232372\n",
      "Loss:  0.551115351356463\n",
      "Loss:  0.5510693733336821\n",
      "Loss:  0.5510234058439925\n",
      "Loss:  0.5509774488768684\n",
      "Loss:  0.5509315024221341\n",
      "Loss:  0.5508855664699361\n",
      "Loss:  0.5508396410107206\n",
      "Loss:  0.5507937260352105\n",
      "Loss:  0.5507478215343851\n",
      "Loss:  0.5507019274994616\n",
      "Loss:  0.5506560439218762\n",
      "Loss:  0.55061017079327\n",
      "Loss:  0.5505643081054717\n",
      "Loss:  0.550518455850486\n",
      "Loss:  0.5504726140204788\n",
      "Loss:  0.5504267826077666\n",
      "Loss:  0.5503809616048052\n",
      "Loss:  0.5503351510041796\n",
      "Loss:  0.5502893507985951\n",
      "Loss:  0.5502435609808671\n",
      "Loss:  0.5501977815439154\n",
      "Loss:  0.5501520124807548\n",
      "Loss:  0.5501062537844893\n",
      "Loss:  0.5500605054483059\n",
      "Loss:  0.5500147674654675\n",
      "Loss:  0.5499690398293096\n",
      "Loss:  0.5499233225332331\n",
      "Loss:  0.5498776155707015\n",
      "Loss:  0.5498319189352348\n",
      "Loss:  0.5497862326204075\n",
      "Loss:  0.5497405566198437\n",
      "Loss:  0.5496948909272137\n",
      "Loss:  0.5496492355362317\n",
      "Loss:  0.5496035904406519\n",
      "Loss:  0.5495579556342667\n",
      "Loss:  0.5495123311109037\n",
      "Loss:  0.5494667168644236\n",
      "Loss:  0.5494211128887179\n",
      "Loss:  0.5493755191777073\n",
      "Loss:  0.5493299357253397\n",
      "Loss:  0.5492843625255881\n",
      "Loss:  0.5492387995724501\n",
      "Loss:  0.5491932468599451\n",
      "Loss:  0.5491477043821142\n",
      "Loss:  0.5491021721330183\n",
      "Loss:  0.5490566501067369\n",
      "Loss:  0.5490111382973676\n",
      "Loss:  0.5489656366990244\n",
      "Loss:  0.548920145305838\n",
      "Loss:  0.5488746641119532\n",
      "Loss:  0.5488291931115294\n",
      "Loss:  0.5487837322987398\n",
      "Loss:  0.5487382816677704\n",
      "Loss:  0.5486928412128191\n",
      "Loss:  0.5486474109280962\n",
      "Loss:  0.5486019908078228\n",
      "Loss:  0.5485565808462306\n",
      "Loss:  0.5485111810375617\n",
      "Loss:  0.5484657913760682\n",
      "Loss:  0.5484204118560113\n",
      "Loss:  0.5483750424716615\n",
      "Loss:  0.5483296832172981\n",
      "Loss:  0.5482843340872088\n",
      "Loss:  0.5482389950756893\n",
      "Loss:  0.5481936661770435\n",
      "Loss:  0.5481483473855826\n",
      "Loss:  0.5481030386956255\n",
      "Loss:  0.5480577401014982\n",
      "Loss:  0.5480124515975336\n",
      "Loss:  0.5479671786881684\n",
      "Loss:  0.5479230452681237\n",
      "Loss:  0.5478789333626615\n",
      "Loss:  0.5478348423381714\n",
      "Loss:  0.547790771606918\n",
      "Loss:  0.5477467206235965\n",
      "Loss:  0.5477026888821463\n",
      "Loss:  0.5476586759128044\n",
      "Loss:  0.5476146812793812\n",
      "Loss:  0.5475707045767398\n",
      "Loss:  0.5475267454284665\n",
      "Loss:  0.5474828034847148\n",
      "Loss:  0.5474388784202133\n",
      "Loss:  0.5473949699324221\n",
      "Loss:  0.5473510777398284\n",
      "Loss:  0.5473072015803707\n",
      "Loss:  0.547263341209981\n",
      "Loss:  0.5472194964012372\n",
      "Loss:  0.5471756669421161\n",
      "Loss:  0.547131852634841\n",
      "Loss:  0.5470880532948166\n",
      "Loss:  0.547044268749642\n",
      "Loss:  0.5470004988381996\n",
      "Loss:  0.5469567434098132\n",
      "Loss:  0.546913002323467\n",
      "Loss:  0.5468692754470855\n",
      "Loss:  0.5468255626568677\n",
      "Loss:  0.5467818638366688\n",
      "Loss:  0.5467381788774324\n",
      "Loss:  0.5466945076766616\n",
      "Loss:  0.5466508501379326\n",
      "Loss:  0.5466072061704434\n",
      "Loss:  0.5465635756885975\n",
      "Loss:  0.5465199586116178\n",
      "Loss:  0.5464763548631904\n",
      "Loss:  0.5464327643711346\n",
      "Loss:  0.5463891870670992\n",
      "Loss:  0.5463456228862789\n",
      "Loss:  0.546302071767155\n",
      "Loss:  0.5462585336512531\n",
      "Loss:  0.5462150084829208\n",
      "Loss:  0.546171496209122\n",
      "Loss:  0.5461279967792446\n",
      "Loss:  0.5460845101449257\n",
      "Loss:  0.5460410362598872\n",
      "Loss:  0.5459975750797863\n",
      "Loss:  0.5459541265620744\n",
      "Loss:  0.5459106906658694\n",
      "Loss:  0.5458672673518352\n",
      "Loss:  0.5458238565820723\n",
      "Loss:  0.5457804583200152\n",
      "Loss:  0.5457370725303374\n",
      "Loss:  0.545693699178865\n",
      "Loss:  0.5456503382324953\n",
      "Loss:  0.5456069896591222\n",
      "Loss:  0.545563653427567\n",
      "Loss:  0.5455203295075141\n",
      "Loss:  0.5454770178694524\n",
      "Loss:  0.5454337184846201\n",
      "Loss:  0.5453904313249545\n",
      "Loss:  0.5453471563630439\n",
      "Loss:  0.5453038935720859\n",
      "Loss:  0.5452606429258459\n",
      "Loss:  0.5452174043986207\n",
      "Loss:  0.5451741779652037\n",
      "Loss:  0.5451309636008537\n",
      "Loss:  0.5450877612812643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5450445709825381\n",
      "Loss:  0.5450013926811599\n",
      "Loss:  0.5449582263539746\n",
      "Loss:  0.544915071978165\n",
      "Loss:  0.5448719295312321\n",
      "Loss:  0.5448287989909757\n",
      "Loss:  0.5447856803354781\n",
      "Loss:  0.5447425735430882\n",
      "Loss:  0.5446994785924064\n",
      "Loss:  0.5446563954622702\n",
      "Loss:  0.5446133241317435\n",
      "Loss:  0.5445702645801027\n",
      "Loss:  0.5445272167868276\n",
      "Loss:  0.54448418073159\n",
      "Loss:  0.5444411563942451\n",
      "Loss:  0.5443981437548226\n",
      "Loss:  0.5443551427935187\n",
      "Loss:  0.5443121534906886\n",
      "Loss:  0.5442691758268398\n",
      "Loss:  0.5442262097826247\n",
      "Loss:  0.5441832553388364\n",
      "Loss:  0.5441403124764014\n",
      "Loss:  0.5440973811763756\n",
      "Loss:  0.5440544614199389\n",
      "Loss:  0.5440115531883908\n",
      "Loss:  0.5439686564631467\n",
      "Loss:  0.5439257712257335\n",
      "Loss:  0.5438828974577866\n",
      "Loss:  0.5438400351410457\n",
      "Loss:  0.5437971842573528\n",
      "Loss:  0.5437543447886479\n",
      "Loss:  0.5437115167169682\n",
      "Loss:  0.5436687000244431\n",
      "Loss:  0.5436258946932944\n",
      "Loss:  0.5435831007058324\n",
      "Loss:  0.5435403180444542\n",
      "Loss:  0.5434975466916423\n",
      "Loss:  0.5434547866299619\n",
      "Loss:  0.5434120378420603\n",
      "Loss:  0.5433693003106643\n",
      "Loss:  0.5433265740185792\n",
      "Loss:  0.5432838589486876\n",
      "Loss:  0.5432411550839473\n",
      "Loss:  0.5431984624073914\n",
      "Loss:  0.5431557809021256\n",
      "Loss:  0.543113110551328\n",
      "Loss:  0.5430704513382484\n",
      "Loss:  0.5430275246893953\n",
      "Loss:  0.5429830429220476\n",
      "Loss:  0.5429391067630942\n",
      "Loss:  0.5428964885434688\n",
      "Loss:  0.5428538805475966\n",
      "Loss:  0.542811282760925\n",
      "Loss:  0.5427686951691225\n",
      "Loss:  0.5427261177580643\n",
      "Loss:  0.5426835505138202\n",
      "Loss:  0.5426409934226427\n",
      "Loss:  0.5425984464709563\n",
      "Loss:  0.5425559096453464\n",
      "Loss:  0.5425133829325517\n",
      "Loss:  0.5424708663194538\n",
      "Loss:  0.5424283597930705\n",
      "Loss:  0.5423858633405475\n",
      "Loss:  0.542343376949152\n",
      "Loss:  0.5423009006062667\n",
      "Loss:  0.5422584342993824\n",
      "Loss:  0.5422159780160941\n",
      "Loss:  0.5421735317440953\n",
      "Loss:  0.5421310954711736\n",
      "Loss:  0.542088669185205\n",
      "Loss:  0.5420462528741521\n",
      "Loss:  0.5420038465260585\n",
      "Loss:  0.5419614501290457\n",
      "Loss:  0.5419190636713108\n",
      "Loss:  0.5418766871411222\n",
      "Loss:  0.5418343205268176\n",
      "Loss:  0.5417919638168011\n",
      "Loss:  0.5417496169995408\n",
      "Loss:  0.541707280063567\n",
      "Loss:  0.5416649529974693\n",
      "Loss:  0.5416226357898954\n",
      "Loss:  0.5415803284295488\n",
      "Loss:  0.5415380309051873\n",
      "Loss:  0.5414957432056214\n",
      "Loss:  0.5414534653197131\n",
      "Loss:  0.5414111972363741\n",
      "Loss:  0.5413689389445648\n",
      "Loss:  0.5413266904332932\n",
      "Loss:  0.5412844516916133\n",
      "Loss:  0.5412422227086247\n",
      "Loss:  0.5412000034734712\n",
      "Loss:  0.5411577939753401\n",
      "Loss:  0.5411155942034612\n",
      "Loss:  0.5410734041471061\n",
      "Loss:  0.5410312237955875\n",
      "Loss:  0.5409890531382583\n",
      "Loss:  0.5409468921645113\n",
      "Loss:  0.540904740863778\n",
      "Loss:  0.5408625992255287\n",
      "Loss:  0.5408204672392714\n",
      "Loss:  0.5407783448945513\n",
      "Loss:  0.5407362321809511\n",
      "Loss:  0.5406941290880896\n",
      "Loss:  0.5406520356056211\n",
      "Loss:  0.5406099517232364\n",
      "Loss:  0.540567877430661\n",
      "Loss:  0.5405258127176551\n",
      "Loss:  0.5404837575740133\n",
      "Loss:  0.5404417119895646\n",
      "Loss:  0.5403996759541718\n",
      "Loss:  0.5403576494577307\n",
      "Loss:  0.5403156324901707\n",
      "Loss:  0.5402736250414538\n",
      "Loss:  0.540231627101575\n",
      "Loss:  0.5401896386605611\n",
      "Loss:  0.5401476597084715\n",
      "Loss:  0.5401056902353969\n",
      "Loss:  0.5400637302314601\n",
      "Loss:  0.5400217796868152\n",
      "Loss:  0.5399798385916472\n",
      "Loss:  0.5399379069361719\n",
      "Loss:  0.5398959847106368\n",
      "Loss:  0.5398540719053188\n",
      "Loss:  0.5398121685105256\n",
      "Loss:  0.5397702745165952\n",
      "Loss:  0.5397283899138955\n",
      "Loss:  0.5396865146928239\n",
      "Loss:  0.5396446488438078\n",
      "Loss:  0.539602792357304\n",
      "Loss:  0.5395609452237983\n",
      "Loss:  0.5395191074338056\n",
      "Loss:  0.5394772789778702\n",
      "Loss:  0.5394354598465649\n",
      "Loss:  0.5393936500304909\n",
      "Loss:  0.5393518495202783\n",
      "Loss:  0.5393100583065852\n",
      "Loss:  0.5392682763800982\n",
      "Loss:  0.5392265037315317\n",
      "Loss:  0.5391847403516279\n",
      "Loss:  0.539142986231157\n",
      "Loss:  0.5391012413609166\n",
      "Loss:  0.5390595057317321\n",
      "Loss:  0.5390177793344559\n",
      "Loss:  0.5389760621599677\n",
      "Loss:  0.5389343541991742\n",
      "Loss:  0.5388926554430092\n",
      "Loss:  0.5388509658824334\n",
      "Loss:  0.5388092855084338\n",
      "Loss:  0.5387676143120241\n",
      "Loss:  0.5387259522842446\n",
      "Loss:  0.5386842994161618\n",
      "Loss:  0.538642655698868\n",
      "Loss:  0.5386010211234822\n",
      "Loss:  0.5385593956811491\n",
      "Loss:  0.5385177793630387\n",
      "Loss:  0.5384761721603475\n",
      "Loss:  0.5384345740642968\n",
      "Loss:  0.5383929850661341\n",
      "Loss:  0.5383514051571314\n",
      "Loss:  0.5383098343285866\n",
      "Loss:  0.5382682725718224\n",
      "Loss:  0.5382267198781865\n",
      "Loss:  0.5381851762390515\n",
      "Loss:  0.5381436416458145\n",
      "Loss:  0.5381021160898977\n",
      "Loss:  0.5380605995627474\n",
      "Loss:  0.5380190920558345\n",
      "Loss:  0.5379775935606543\n",
      "Loss:  0.5379361040687263\n",
      "Loss:  0.5378946235715937\n",
      "Loss:  0.537853152060824\n",
      "Loss:  0.5378116895280085\n",
      "Loss:  0.5377702359647621\n",
      "Loss:  0.5377287913627237\n",
      "Loss:  0.5376873557135553\n",
      "Loss:  0.5376459290089428\n",
      "Loss:  0.5376045112405949\n",
      "Loss:  0.5375631024002441\n",
      "Loss:  0.5375217024796454\n",
      "Loss:  0.5374803114705772\n",
      "Loss:  0.5374389293648407\n",
      "Loss:  0.5373975561542602\n",
      "Loss:  0.5373561918306823\n",
      "Loss:  0.5373148363859761\n",
      "Loss:  0.5372734898120337\n",
      "Loss:  0.5372321521007691\n",
      "Loss:  0.5371908232441192\n",
      "Loss:  0.5371495032340421\n",
      "Loss:  0.5371081920625194\n",
      "Loss:  0.5370668897215533\n",
      "Loss:  0.5370255962031687\n",
      "Loss:  0.5369843114994121\n",
      "Loss:  0.5369430356023518\n",
      "Loss:  0.5369017685040777\n",
      "Loss:  0.5368605101967008\n",
      "Loss:  0.5368192606723541\n",
      "Loss:  0.5367780199231919\n",
      "Loss:  0.536736787941389\n",
      "Loss:  0.536695564719142\n",
      "Loss:  0.5366543502486685\n",
      "Loss:  0.5366131445222069\n",
      "Loss:  0.5365719475320161\n",
      "Loss:  0.5365307592703764\n",
      "Loss:  0.5364895797295883\n",
      "Loss:  0.5364484089019733\n",
      "Loss:  0.5364072467798728\n",
      "Loss:  0.536366093355649\n",
      "Loss:  0.5363249486216842\n",
      "Loss:  0.5362838125703812\n",
      "Loss:  0.5362426851941624\n",
      "Loss:  0.5362015664854708\n",
      "Loss:  0.5361604564367688\n",
      "Loss:  0.5361193550405392\n",
      "Loss:  0.5360782622892841\n",
      "Loss:  0.5360371781755257\n",
      "Loss:  0.535996102691805\n",
      "Loss:  0.5359550358306835\n",
      "Loss:  0.5359139775847416\n",
      "Loss:  0.5358729279465788\n",
      "Loss:  0.5358318869088146\n",
      "Loss:  0.5357908544640866\n",
      "Loss:  0.5357498306050522\n",
      "Loss:  0.5357088153243879\n",
      "Loss:  0.5356678086147886\n",
      "Loss:  0.5356268104689683\n",
      "Loss:  0.5355864532557704\n",
      "Loss:  0.5355458666062791\n",
      "Loss:  0.5355053033312721\n",
      "Loss:  0.5354649918117478\n",
      "Loss:  0.5354245148732819\n",
      "Loss:  0.5353840053382651\n",
      "Loss:  0.5353435819367588\n",
      "Loss:  0.5353033085113956\n",
      "Loss:  0.535262850073442\n",
      "Loss:  0.5352224122734044\n",
      "Loss:  0.5351821055638786\n",
      "Loss:  0.5351418372426904\n",
      "Loss:  0.5351014472711104\n",
      "Loss:  0.5350610767326014\n",
      "Loss:  0.5350208124335859\n",
      "Loss:  0.5349806167802101\n",
      "Loss:  0.5349402915272519\n",
      "Loss:  0.5348999847439933\n",
      "Loss:  0.5348596961048522\n",
      "Loss:  0.534819635538167\n",
      "Loss:  0.5347793764090449\n",
      "Loss:  0.5347391303928087\n",
      "Loss:  0.5346989018037691\n",
      "Loss:  0.5346587320876952\n",
      "Loss:  0.5346186866764486\n",
      "Loss:  0.5345784992482339\n",
      "Loss:  0.5345383286567621\n",
      "Loss:  0.5344981746755748\n",
      "Loss:  0.534458037092169\n",
      "Loss:  0.5344180890700329\n",
      "Loss:  0.5343779794313609\n",
      "Loss:  0.5343378809687019\n",
      "Loss:  0.5342977984857955\n",
      "Loss:  0.5342577318048638\n",
      "Loss:  0.5342177031450893\n",
      "Loss:  0.5341777986975177\n",
      "Loss:  0.5341377699669062\n",
      "Loss:  0.5340977567075184\n",
      "Loss:  0.5340577587674995\n",
      "Loss:  0.5340177760035345\n",
      "Loss:  0.5339778082802253\n",
      "Loss:  0.5339379249528756\n",
      "Loss:  0.5338980484997888\n",
      "Loss:  0.5338581172774812\n",
      "Loss:  0.5338182008666\n",
      "Loss:  0.5337782991473577\n",
      "Loss:  0.5337384120062032\n",
      "Loss:  0.5336985393353707\n",
      "Loss:  0.5336586810324627\n",
      "Loss:  0.5336189105883712\n",
      "Loss:  0.533579114079892\n",
      "Loss:  0.5335392910198172\n",
      "Loss:  0.5334994821717112\n",
      "Loss:  0.5334596874413142\n",
      "Loss:  0.5334199067387952\n",
      "Loss:  0.533380139978435\n",
      "Loss:  0.5333403870783334\n",
      "Loss:  0.533300647960137\n",
      "Loss:  0.5332609225487885\n",
      "Loss:  0.5332212670085389\n",
      "Loss:  0.5331815908677673\n",
      "Loss:  0.5331418994792223\n",
      "Loss:  0.5331022216955518\n",
      "Loss:  0.5330625574443107\n",
      "Loss:  0.5330229066559717\n",
      "Loss:  0.532983269263719\n",
      "Loss:  0.5329436452032597\n",
      "Loss:  0.5329040344126471\n",
      "Loss:  0.5328644368321191\n",
      "Loss:  0.5328248524039462\n",
      "Loss:  0.5327852810722928\n",
      "Loss:  0.5327457227830875\n",
      "Loss:  0.5327061774839036\n",
      "Loss:  0.5326666451238482\n",
      "Loss:  0.5326271256534593\n",
      "Loss:  0.5325876488349115\n",
      "Loss:  0.5325481599103727\n",
      "Loss:  0.5325086729670889\n",
      "Loss:  0.5324691988504149\n",
      "Loss:  0.5324297375095085\n",
      "Loss:  0.5323902888950099\n",
      "Loss:  0.5323508529589416\n",
      "Loss:  0.5323114296546174\n",
      "Loss:  0.5322720189365563\n",
      "Loss:  0.5322326207604045\n",
      "Loss:  0.5321932350828616\n",
      "Loss:  0.532153861861614\n",
      "Loss:  0.5321145010552708\n",
      "Loss:  0.532075152623307\n",
      "Loss:  0.532035816526008\n",
      "Loss:  0.5319964927244217\n",
      "Loss:  0.5319571811803109\n",
      "Loss:  0.5319178818561106\n",
      "Loss:  0.5318785947148886\n",
      "Loss:  0.5318393197203084\n",
      "Loss:  0.5318000568365948\n",
      "Loss:  0.5317608060285031\n",
      "Loss:  0.531721567261288\n",
      "Loss:  0.5316823405006789\n",
      "Loss:  0.5316431257128518\n",
      "Loss:  0.5316039228644074\n",
      "Loss:  0.5315647319223498\n",
      "Loss:  0.5315255528540648\n",
      "Loss:  0.5314863856273021\n",
      "Loss:  0.5314472302101578\n",
      "Loss:  0.5314080865710576\n",
      "Loss:  0.531368954678743\n",
      "Loss:  0.5313298345022556\n",
      "Loss:  0.531290726010926\n",
      "Loss:  0.5312516291743599\n",
      "Loss:  0.5312125439624282\n",
      "Loss:  0.5311734703452564\n",
      "Loss:  0.5311344082932141\n",
      "Loss:  0.5310953577769066\n",
      "Loss:  0.5310563187671663\n",
      "Loss:  0.5310172912350452\n",
      "Loss:  0.5309782751518066\n",
      "Loss:  0.5309392704889195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5309002772180513\n",
      "Loss:  0.5308612953110627\n",
      "Loss:  0.5308223247400011\n",
      "Loss:  0.5307833654770965\n",
      "Loss:  0.5307444174947558\n",
      "Loss:  0.5307054807655589\n",
      "Loss:  0.530666555262254\n",
      "Loss:  0.5306276409577542\n",
      "Loss:  0.5305887378251326\n",
      "Loss:  0.5305498458376208\n",
      "Loss:  0.5305109649686033\n",
      "Loss:  0.5304720951916162\n",
      "Loss:  0.5304332364803435\n",
      "Loss:  0.5303943888086146\n",
      "Loss:  0.5303555521504014\n",
      "Loss:  0.5303167264798168\n",
      "Loss:  0.5302779117711108\n",
      "Loss:  0.5302391079986706\n",
      "Loss:  0.530200315137016\n",
      "Loss:  0.5301615331607998\n",
      "Loss:  0.530122762044804\n",
      "Loss:  0.5300840017639399\n",
      "Loss:  0.530045252293245\n",
      "Loss:  0.5300065136078819\n",
      "Loss:  0.5299677856831373\n",
      "Loss:  0.52992906849442\n",
      "Loss:  0.5298903620172598\n",
      "Loss:  0.5298516662273057\n",
      "Loss:  0.5298129811003257\n",
      "Loss:  0.5297743066122048\n",
      "Loss:  0.5297356427389437\n",
      "Loss:  0.5296969894566587\n",
      "Loss:  0.5296583467415793\n",
      "Loss:  0.5296197145700485\n",
      "Loss:  0.5295810929185211\n",
      "Loss:  0.5295424817635626\n",
      "Loss:  0.529503881081849\n",
      "Loss:  0.5294652908501651\n",
      "Loss:  0.5294267110454044\n",
      "Loss:  0.5293881416445678\n",
      "Loss:  0.5293495826247627\n",
      "Loss:  0.529311033963203\n",
      "Loss:  0.5292724956372072\n",
      "Loss:  0.5292339676241988\n",
      "Loss:  0.5291954499017045\n",
      "Loss:  0.5291569424473543\n",
      "Loss:  0.5291184452388805\n",
      "Loss:  0.5290799582541172\n",
      "Loss:  0.5290414814709994\n",
      "Loss:  0.5290030148675624\n",
      "Loss:  0.5289645584219415\n",
      "Loss:  0.5289261121123708\n",
      "Loss:  0.5288876759171832\n",
      "Loss:  0.5288492498148095\n",
      "Loss:  0.5288108337837777\n",
      "Loss:  0.5287724278027125\n",
      "Loss:  0.5287340318503352\n",
      "Loss:  0.5286956459054621\n",
      "Loss:  0.5286572699470053\n",
      "Loss:  0.5286189039539709\n",
      "Loss:  0.5285805479054594\n",
      "Loss:  0.5285422017806644\n",
      "Loss:  0.5285038655588726\n",
      "Loss:  0.5284655392194633\n",
      "Loss:  0.5284272227419076\n",
      "Loss:  0.528388916105768\n",
      "Loss:  0.5283506192906978\n",
      "Loss:  0.5283123322764411\n",
      "Loss:  0.5282740550428316\n",
      "Loss:  0.5282357875697927\n",
      "Loss:  0.5281975298373365\n",
      "Loss:  0.5281592818255639\n",
      "Loss:  0.5281210435146634\n",
      "Loss:  0.5280828148849117\n",
      "Loss:  0.528044595916672\n",
      "Loss:  0.5280063865903944\n",
      "Loss:  0.5279681868866153\n",
      "Loss:  0.5279299967859565\n",
      "Loss:  0.5278918162691256\n",
      "Loss:  0.5278536453169144\n",
      "Loss:  0.5278154839101996\n",
      "Loss:  0.5277773320299418\n",
      "Loss:  0.5277391896571848\n",
      "Loss:  0.5277010567730558\n",
      "Loss:  0.5276629333587645\n",
      "Loss:  0.5276248193956029\n",
      "Loss:  0.527586714864945\n",
      "Loss:  0.5275486197482459\n",
      "Loss:  0.5275105340270418\n",
      "Loss:  0.5274724576829495\n",
      "Loss:  0.527434390697666\n",
      "Loss:  0.5273963330529675\n",
      "Loss:  0.5273582847307106\n",
      "Loss:  0.5273202457128298\n",
      "Loss:  0.5272822159813386\n",
      "Loss:  0.5272441955183286\n",
      "Loss:  0.527206184305969\n",
      "Loss:  0.5271681823265068\n",
      "Loss:  0.5271301895622649\n",
      "Loss:  0.5270922059956439\n",
      "Loss:  0.5270542316091198\n",
      "Loss:  0.5270162663852448\n",
      "Loss:  0.5269783103066462\n",
      "Loss:  0.5269403633560265\n",
      "Loss:  0.5269024255161627\n",
      "Loss:  0.5268644967699061\n",
      "Loss:  0.5268265771001819\n",
      "Loss:  0.5267886664899886\n",
      "Loss:  0.5267507649223983\n",
      "Loss:  0.5267128723805554\n",
      "Loss:  0.5266749888476768\n",
      "Loss:  0.5266371143070516\n",
      "Loss:  0.5265992487420403\n",
      "Loss:  0.5265613921360751\n",
      "Loss:  0.5265235444726587\n",
      "Loss:  0.5264857057353646\n",
      "Loss:  0.5264478759078364\n",
      "Loss:  0.5264100549737877\n",
      "Loss:  0.5263722429170017\n",
      "Loss:  0.5263344397213305\n",
      "Loss:  0.5262966453706955\n",
      "Loss:  0.5262588598490859\n",
      "Loss:  0.5262210831405596\n",
      "Loss:  0.526183315229242\n",
      "Loss:  0.526145556099326\n",
      "Loss:  0.5261078057350719\n",
      "Loss:  0.5260700641208063\n",
      "Loss:  0.5260323312409225\n",
      "Loss:  0.5259946070798798\n",
      "Loss:  0.5259568916222033\n",
      "Loss:  0.5259191848524836\n",
      "Loss:  0.5258814867553764\n",
      "Loss:  0.5258437973156023\n",
      "Loss:  0.525806116517946\n",
      "Loss:  0.5257684443472564\n",
      "Loss:  0.5257307807884468\n",
      "Loss:  0.5256931258264932\n",
      "Loss:  0.5256554794464352\n",
      "Loss:  0.5256178416333753\n",
      "Loss:  0.5255802123724783\n",
      "Loss:  0.5255425916489713\n",
      "Loss:  0.5255049794481434\n",
      "Loss:  0.5254673757553454\n",
      "Loss:  0.525429780555989\n",
      "Loss:  0.5253921938355474\n",
      "Loss:  0.5253546155795539\n",
      "Loss:  0.5253170457736027\n",
      "Loss:  0.5252794844033477\n",
      "Loss:  0.525241931454503\n",
      "Loss:  0.5252043869128418\n",
      "Loss:  0.5251668507641964\n",
      "Loss:  0.5251293229944585\n",
      "Loss:  0.5250918035895775\n",
      "Loss:  0.5250542925355621\n",
      "Loss:  0.5250167898184783\n",
      "Loss:  0.5249792954244501\n",
      "Loss:  0.5249418093396587\n",
      "Loss:  0.5249043315503428\n",
      "Loss:  0.5248668620427975\n",
      "Loss:  0.5248294008033747\n",
      "Loss:  0.5247919478184826\n",
      "Loss:  0.5247545030745853\n",
      "Loss:  0.5247170665582025\n",
      "Loss:  0.5246796382559096\n",
      "Loss:  0.524642218154337\n",
      "Loss:  0.5246048062401699\n",
      "Loss:  0.5245674025001482\n",
      "Loss:  0.5245300069210662\n",
      "Loss:  0.5244926194897723\n",
      "Loss:  0.5244552401931684\n",
      "Loss:  0.5244178690182099\n",
      "Loss:  0.5243805059519059\n",
      "Loss:  0.524343150981318\n",
      "Loss:  0.5243058040935612\n",
      "Loss:  0.5242684652758017\n",
      "Loss:  0.5242311345152594\n",
      "Loss:  0.5241938117992049\n",
      "Loss:  0.524156497114961\n",
      "Loss:  0.5241191904499022\n",
      "Loss:  0.5240818917914536\n",
      "Loss:  0.5240446011270915\n",
      "Loss:  0.5240073184443427\n",
      "Loss:  0.5239700437307844\n",
      "Loss:  0.523932776974044\n",
      "Loss:  0.5238955181617989\n",
      "Loss:  0.5238582672817758\n",
      "Loss:  0.5238210243217513\n",
      "Loss:  0.5237837892695505\n",
      "Loss:  0.5237465621130479\n",
      "Loss:  0.5237093428401665\n",
      "Loss:  0.5236721314388776\n",
      "Loss:  0.5236349278972007\n",
      "Loss:  0.5235977322032034\n",
      "Loss:  0.5235605443450009\n",
      "Loss:  0.5235233643107556\n",
      "Loss:  0.5234861920886775\n",
      "Loss:  0.5234490276670234\n",
      "Loss:  0.5234118710340965\n",
      "Loss:  0.5233747221782472\n",
      "Loss:  0.5233375810878717\n",
      "Loss:  0.5233004477514122\n",
      "Loss:  0.523263322157357\n",
      "Loss:  0.5232262042942398\n",
      "Loss:  0.5231890941506396\n",
      "Loss:  0.5231519917151808\n",
      "Loss:  0.5231148969765321\n",
      "Loss:  0.5230778099234078\n",
      "Loss:  0.5230407305445658\n",
      "Loss:  0.5230036588288087\n",
      "Loss:  0.522966594764983\n",
      "Loss:  0.5229295383419789\n",
      "Loss:  0.5228924895487304\n",
      "Loss:  0.5228554483742143\n",
      "Loss:  0.5228184148074515\n",
      "Loss:  0.5227813888375049\n",
      "Loss:  0.5227443704534808\n",
      "Loss:  0.5227073596445272\n",
      "Loss:  0.5226703563998353\n",
      "Loss:  0.5226333607086375\n",
      "Loss:  0.522596372560209\n",
      "Loss:  0.5225593919438657\n",
      "Loss:  0.5225224188489654\n",
      "Loss:  0.5224854532649077\n",
      "Loss:  0.522448495181132\n",
      "Loss:  0.5224115445871196\n",
      "Loss:  0.5223746014723918\n",
      "Loss:  0.5223376658265105\n",
      "Loss:  0.522300737639078\n",
      "Loss:  0.5222638168997367\n",
      "Loss:  0.5222269035981683\n",
      "Loss:  0.5221899977240942\n",
      "Loss:  0.522153099267276\n",
      "Loss:  0.5221162082175137\n",
      "Loss:  0.5220793245646467\n",
      "Loss:  0.5220424482985531\n",
      "Loss:  0.5220055794091495\n",
      "Loss:  0.5219687178863914\n",
      "Loss:  0.5219318637202723\n",
      "Loss:  0.5218950169008237\n",
      "Loss:  0.5218581774181151\n",
      "Loss:  0.5218213452622538\n",
      "Loss:  0.5217845204233842\n",
      "Loss:  0.5217477028916884\n",
      "Loss:  0.5217108926573854\n",
      "Loss:  0.5216740897107315\n",
      "Loss:  0.5216372940420192\n",
      "Loss:  0.5216005056415781\n",
      "Loss:  0.5215637237313843\n",
      "Loss:  0.521526929057722\n",
      "Loss:  0.5214901415724947\n",
      "Loss:  0.5214533612661736\n",
      "Loss:  0.5214165881292651\n",
      "Loss:  0.5213798221523116\n",
      "Loss:  0.5213430633258908\n",
      "Loss:  0.521306311640615\n",
      "Loss:  0.5212695670871325\n",
      "Loss:  0.5212328296561257\n",
      "Loss:  0.5211960993383117\n",
      "Loss:  0.5211593761244424\n",
      "Loss:  0.5211226600053037\n",
      "Loss:  0.5210859509717162\n",
      "Loss:  0.5210492490145335\n",
      "Loss:  0.521012554124644\n",
      "Loss:  0.5209758662929691\n",
      "Loss:  0.5209391855104639\n",
      "Loss:  0.5209025117681167\n",
      "Loss:  0.5208658450569492\n",
      "Loss:  0.5208291853680158\n",
      "Loss:  0.5207925326924038\n",
      "Loss:  0.520755887021233\n",
      "Loss:  0.5207192483456557\n",
      "Loss:  0.5206826166568568\n",
      "Loss:  0.5206459919460533\n",
      "Loss:  0.5206093742044939\n",
      "Loss:  0.5205727634234594\n",
      "Loss:  0.520536159594262\n",
      "Loss:  0.520499562708246\n",
      "Loss:  0.5204629727567864\n",
      "Loss:  0.5204263897312897\n",
      "Loss:  0.5203898136231938\n",
      "Loss:  0.5203532444239668\n",
      "Loss:  0.5203166821251081\n",
      "Loss:  0.5202801267181474\n",
      "Loss:  0.5202435781946451\n",
      "Loss:  0.5202070365461917\n",
      "Loss:  0.5201705017644079\n",
      "Loss:  0.5201339738409446\n",
      "Loss:  0.5200974527674821\n",
      "Loss:  0.5200609385357307\n",
      "Loss:  0.5200244311374302\n",
      "Loss:  0.5199879305643501\n",
      "Loss:  0.5199514368082884\n",
      "Loss:  0.5199149498610731\n",
      "Loss:  0.5198784697145606\n",
      "Loss:  0.5198419963606364\n",
      "Loss:  0.5198055297912145\n",
      "Loss:  0.5197690699982374\n",
      "Loss:  0.5197326169736765\n",
      "Loss:  0.5196961707095308\n",
      "Loss:  0.5196597311978277\n",
      "Loss:  0.5196232984306229\n",
      "Loss:  0.5195868723999995\n",
      "Loss:  0.5195504530980684\n",
      "Loss:  0.5195140405169681\n",
      "Loss:  0.5194776346488649\n",
      "Loss:  0.5194412354859517\n",
      "Loss:  0.5194048430204492\n",
      "Loss:  0.5193684572446047\n",
      "Loss:  0.5193320781506928\n",
      "Loss:  0.5192957057310146\n",
      "Loss:  0.5192593399778977\n",
      "Loss:  0.5192229808836964\n",
      "Loss:  0.5191866284407919\n",
      "Loss:  0.5191502826415902\n",
      "Loss:  0.5191139434785252\n",
      "Loss:  0.5190776109440554\n",
      "Loss:  0.5190412850306657\n",
      "Loss:  0.5190049657308669\n",
      "Loss:  0.518968653037195\n",
      "Loss:  0.5189323469422117\n",
      "Loss:  0.5188960474385043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78.75"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork(structure).initialize_weights()\n",
    "# nn.show_model()\n",
    "\n",
    "for _ in range(1000):\n",
    "    nn.forward_prop(X)\n",
    "    print(\"Loss: \", np.sum(Losses.binary_cross_entropy(Y, nn.net_params[\"a\" + str(nn.num_layers)])))\n",
    "    nn.back_prop(X, Y, \"binary_cross_entropy\")\n",
    "    nn.update_weights(X, 0.001)\n",
    "nn.forward_prop(X)\n",
    "nn.net_params[\"a3\"]\n",
    "nn.accuracy(X, Y)\n",
    "# nn.show_model()\n",
    "# nn.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "96979b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.67266895e-02,  2.76882095e-04,  1.97660053e-02,  8.43704860e-04,\n",
       "       -7.41977294e-02,  3.54517817e-02, -5.28151063e-04,  5.80279796e-05,\n",
       "        2.58365395e-04,  5.87351290e-04, -9.91657687e-03, -3.64000021e-02])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(nn.net_params[\"delta1\"], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "643c6895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_data(n = 40):\n",
    "    X1_1 = 2 + 4 * np.random.rand(n, 1)\n",
    "    X1_2 = 1 + 4 * np.random.rand(n, 1)\n",
    "    class1 = np.concatenate((X1_1, X1_2), axis=1)\n",
    "    Y1 = np.ones((n, 1))\n",
    "\n",
    "    # Class 0 - samples generation\n",
    "    X0_1 = 4 + 4 * np.random.rand(n, 1)\n",
    "    X0_2 = 3 + 4 * np.random.rand(n, 1)\n",
    "    class0 = np.concatenate((X0_1, X0_2), axis=1)\n",
    "    Y0 = np.zeros((n, 1))\n",
    "\n",
    "    X = np.concatenate((class1, class0))\n",
    "    Y = np.concatenate((Y1, Y0))\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def augment_data(data, rand_scale = 0.1):\n",
    "    return data + rand_scale * np.random.randn(*data.shape)\n",
    "\n",
    "\n",
    "# running Task 2\n",
    "# Read data\n",
    "\n",
    "X, Y = gen_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "230a041f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 1, 0, 0, 1])\n",
    "y = np.array([1, 1, 1, 0, 0])\n",
    "\n",
    "sum(x == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "83f0dca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8875*80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e74ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
